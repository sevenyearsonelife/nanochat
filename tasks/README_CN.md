# NanoChat 评估任务详解

本目录包含了 NanoChat 项目的所有评估任务，用于测试和验证大语言模型在不同领域的表现。

`★ Insight ─────────────────────────────────────`
• tasks目录是模型评估的核心组件，涵盖学术基准测试和实用技能评估
• 所有任务都继承自统一的Task基类，确保一致的接口和评估流程
• 任务设计支持生成式和分类式两种评估类型，适配不同的能力测试需求
`─────────────────────────────────────────────────`

## 任务架构概览

### 基础框架 (`common.py`)

**common.py** 定义了所有任务的基类和通用工具：

- **Task基类**: 所有评估任务的基础抽象类
  - 支持数据集切片（start, stop, step参数）
  - 定义了评估类型（'generative'生成式 或 'categorical'分类式）
  - 提供统一的评估接口

- **TaskMixture类**: 混合多个任务进行训练
  - 支持按比例混合不同数据集
  - 适用于监督微调（SFT）阶段

- **TaskSequence类**: 顺序执行多个任务
  - 支持课程学习（curriculum learning）
  - 按指定顺序依次训练各个任务

- **render_mc函数**: 标准化多选题格式渲染
  - 统一的多选题展示格式
  - 优化tokenization，避免空格导致的token不匹配

## 具体任务详解

### 1. GSM8K (`gsm8k.py`) - 小学数学推理

**任务描述**: 测试模型的小学级别数学问题解决能力

**数据特点**:
- 包含数学应用题和详细解答步骤
- 支持工具调用（计算器）格式
- 包含main和socratic两种子集

**评估方式**: 生成式
- 提取####标记后的数值答案
- 支持Python工具调用解析
- 严格的数值匹配验证

**示例问题**:
```
Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
```

### 2. HumanEval (`humaneval.py`) - Python编程能力

**任务描述**: 评估Python函数编程和代码生成能力

**数据特点**:
- 包含函数签名和文档字符串
- 提供标准解答和测试用例
- 需要完整实现函数功能

**评估方式**: 生成式
- 执行生成的代码进行验证
- 支持多种代码格式解析
- 自动提取导入语句并补充

**核心功能**:
- `extract_program()`: 从LLM输出中提取Python代码
- `execute_code()`: 安全的代码执行环境
- 自动化测试用例验证

### 3. MMLU (`mmlu.py`) - 多领域知识理解

**任务描述**: 大规模多任务语言理解基准测试

**覆盖领域**: 57个不同学科，包括：
- STEM学科：数学、物理、化学、计算机科学
- 人文学科：历史、哲学、法律
- 社会科学：心理学、经济学、社会学

**评估方式**: 分类式（四选一）
- 标准化多选题格式
- 字母答案验证（A、B、C、D）
- 支持按学科分组评估

**数据子集**:
- `all`: 完整验证集和测试集
- `auxiliary_train`: 辅助训练数据

### 4. ARC (`arc.py`) - 科学推理能力

**任务描述**: AI2的科学推理挑战，评估科学知识理解

**数据集**:
- **ARC-Easy**: 较简单的科学问题
- **ARC-Challenge**: 更具挑战性的科学问题

**评估方式**: 分类式
- 支持不固定数量的选项（通常3-4个）
- 动态字母标签系统
- 科学概念深度理解测试

### 5. SmolTalk (`smoltalk.py`) - 日常对话能力

**任务描述**: HuggingFace的通用对话数据集

**数据特点**:
- 训练集：460K对话样本
- 测试集：24K对话样本
- 真实多样的对话场景
- 支持系统提示（system message）

**评估特点**:
- 专注于对话流畅性和连贯性
- 面向较小模型的轻量化版本
- 严格的消息交替验证

### 6. SpellingBee (`spellingbee.py`) - 拼写和计数技能

**任务描述**: 专注于字母计数和单词拼写的细粒度技能训练

**包含两个子任务**:

#### 6.1 SpellingBee（字母计数）
- **问题类型**: "How many 'r' are in strawberry?" → 3
- **训练策略**: 结合手动计数和Python验证
- **数据增强**:
  - 100+种用户问题模板
  - 多语言支持（英、中、西、韩、法、德、日）
  - 随机大小写和引号变化

#### 6.2 SimpleSpelling（单词拼写）
- **任务**: 单词拼写训练
- **格式**: "spell: word" → "word: w,o,r,d"
- **目的**: 强化token到字符的映射能力

**教育价值**: 帮助小模型学习细致的语言操作技能

### 7. CustomJSON (`customjson.py`) - 自定义对话数据

**任务描述**: 支持从JSONL文件加载自定义对话数据

**文件格式**:
```json
{"role":"user","content":"你好"}
{"role":"assistant","content":"你好！有什么可以帮助你的？"}
```

**验证机制**:
- 消息角色交替检查
- 内容字段完整性验证
- 文件存在性检查

**使用场景**:
- 加载身份定制对话
- 集成特定领域对话数据
- 支持个性化训练

## 评估类型说明

### 生成式评估 (Generative)
- **适用任务**: GSM8K, HumanEval, SpellingBee
- **特点**: 需要生成完整答案，进行内容匹配
- **验证**: 通常需要解析特定格式或执行代码

### 分类式评估 (Categorical)
- **适用任务**: MMLU, ARC
- **特点**: 从预定义选项中选择答案
- **验证**: 简单的选项匹配，适合大规模评估

## 任务使用方法

### 基本用法
```python
from tasks.gsm8k import GSM8K

# 创建任务实例
task = GSM8K(subset="main", split="test")

# 获取样本
example = task[0]

# 评估回答
result = task.evaluate(conversation, response)
```

### 任务混合
```python
from tasks.common import TaskMixture

# 混合多个任务
mixed_task = TaskMixture([task1, task2, task3])
```

### 数据切片
```python
# 使用部分数据进行快速测试
subset = GSM8K(subset="main", split="test", start=0, stop=100)
```

## 技术特点

1. **统一接口**: 所有任务遵循相同的API规范
2. **灵活切片**: 支持任意范围的数据子集访问
3. **确定性随机**: 使用种子确保可重现的结果
4. **多语言支持**: 部分任务支持多种语言模板
5. **工具调用集成**: 支持Python代码执行和计算器使用

这些评估任务构成了NanoChat完整的评估体系，涵盖了从基础语言理解到复杂推理编程的多维度能力测试。