# Chat Dialogue Log

## å¯¹è¯æ—¶é—´
2025-11-12 14:47:26

---

## å¯¹è¯å†…å®¹

### é—®é¢˜ 1
ç”¨æˆ·ï¼šè¯·è¯¦ç»†è§£é‡Šrustbpeæ–‡ä»¶å¤¹çš„åŠŸèƒ½!
åŠ©æ‰‹ï¼šæˆ‘å°†è¯¦ç»†è§£é‡Š rustbpe æ–‡ä»¶å¤¹çš„åŠŸèƒ½ã€‚è®©æˆ‘å…ˆæ¢ç´¢è¿™ä¸ªæ–‡ä»¶å¤¹çš„ç»“æ„å’Œå†…å®¹ã€‚

rustbpe æ˜¯ nanochat é¡¹ç›®ä¸­çš„æ ¸å¿ƒåˆ†è¯å™¨è®­ç»ƒæ¨¡å—ï¼Œå®ƒå®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„ **Byte Pair Encoding (BPE)** åˆ†è¯å™¨ã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šå…¶åŠŸèƒ½å’Œæ¶æ„ï¼š

### ğŸ“ æ–‡ä»¶å¤¹ç»“æ„

```
rustbpe/
â”œâ”€â”€ Cargo.toml          # Rust é¡¹ç›®é…ç½®å’Œä¾èµ–
â”œâ”€â”€ Cargo.lock          # ä¾èµ–ç‰ˆæœ¬é”å®šæ–‡ä»¶
â”œâ”€â”€ src/lib.rs          # æ ¸å¿ƒ Rust å®ç°
â”œâ”€â”€ README.md           # è‹±æ–‡æ–‡æ¡£
â””â”€â”€ README_CN.md        # ä¸­æ–‡æ–‡æ¡£
```

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½è®¾è®¡

`rustbpe` å¡«è¡¥äº†ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯ç©ºç™½ï¼šç°æœ‰çš„ tiktoken åº“åªæ”¯æŒæ¨ç†ï¼Œè€Œ HuggingFace tokenizers åº“è¿‡äºå¤æ‚ã€‚rustbpe æä¾›äº†ï¼š

- **è®­ç»ƒåŠŸèƒ½**: é«˜æ•ˆçš„ BPE åˆ†è¯å™¨è®­ç»ƒ
- **ç®€æ´æ€§**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- **æ€§èƒ½**: æ¯” Python å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦
- **å…¼å®¹æ€§**: å¯¼å‡ºçš„è¯æ±‡è¡¨å¯ç›´æ¥ç”¨äº tiktoken æ¨ç†

### âš™ï¸ æŠ€æœ¯å®ç°ç‰¹ç‚¹

#### 1. **GPT-4 é£æ ¼çš„åˆ†è¯**
```rust
const GPT4_PATTERN: &str = r"'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+";
```
- ä½¿ç”¨ä¸ GPT-4 ç›¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†
- å°†æ–‡æœ¬åˆ†å‰²æˆé€‚åˆ BPE å¤„ç†çš„å•å…ƒ

#### 2. **é«˜æ•ˆå¹¶è¡Œè®­ç»ƒ**
- ä½¿ç”¨ **Rayon** åº“å®ç°æ•°æ®å¹¶è¡Œå¤„ç†
- å…«å‰å † (`OctonaryHeap`) ä¼˜åŒ–åˆå¹¶æ“ä½œçš„æ€§èƒ½
- å¢é‡å¼è®­ç»ƒç®—æ³•ï¼Œé¿å…é‡å¤è®¡ç®—

#### 3. **å†…å­˜ä¼˜åŒ–è®¾è®¡**
```rust
pub struct Tokenizer {
    pub merges: StdHashMap<Pair, u32>,  // åˆå¹¶è§„åˆ™æ˜ å°„
    pub pattern: String,                // åˆ†è¯æ¨¡å¼
    compiled_pattern: Regex,            // é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
}
```
- ç´§å‡‘çš„æ•°æ®ç»“æ„è®¾è®¡
- é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æé«˜è¿è¡Œæ•ˆç‡
- ä½¿ç”¨ `CompactString` å’Œ `AHashMap` ä¼˜åŒ–å†…å­˜ä½¿ç”¨

### ğŸ”§ æ ¸å¿ƒç®—æ³•æµç¨‹

#### BPE è®­ç»ƒè¿‡ç¨‹ï¼š
1. **æ–‡æœ¬é¢„å¤„ç†**: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬åˆ†å‰²æˆå°å—
2. **å­—èŠ‚çº§ç¼–ç **: å°†æ¯ä¸ªå­—ç¬¦è½¬æ¢ä¸ºå­—èŠ‚ (0-255)
3. **é¢‘ç‡ç»Ÿè®¡**: ç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡
4. **è¿­ä»£åˆå¹¶**: é‡å¤é€‰æ‹©æœ€é«˜é¢‘çš„å­—èŠ‚å¯¹è¿›è¡Œåˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°

#### å…³é”®ä¼˜åŒ–ï¼š
- **å¹¶è¡Œç»Ÿè®¡**: ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—å­—èŠ‚å¯¹é¢‘ç‡
- **å¢é‡æ›´æ–°**: åªé‡æ–°è®¡ç®—å—å½±å“çš„å­—èŠ‚å¯¹é¢‘ç‡
- **å †ä¼˜åŒ–**: ä½¿ç”¨ä¼˜å…ˆå †å¿«é€Ÿæ‰¾åˆ°æœ€é«˜é¢‘å­—èŠ‚å¯¹

### ğŸ Python é›†æˆæ–¹å¼

rustbpe é€šè¿‡ **PyO3** æ¡†æ¶ä¸ Python æ— ç¼é›†æˆï¼š

```python
# nanochat/tokenizer.py:164-182
class RustBPETokenizer:
    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        # 1) ä½¿ç”¨ rustbpe è®­ç»ƒ
        tokenizer = rustbpe.Tokenizer()
        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)

        # 2) æ„å»º tiktoken ç¼–ç å™¨ç”¨äºæ¨ç†
        mergeable_ranks = {bytes(k): v for k, v in tokenizer.get_mergeable_ranks()}
        enc = tiktoken.Encoding(
            name="rustbpe",
            pat_str=pattern,
            mergeable_ranks=mergeable_ranks,
            special_tokens=special_tokens,
        )
        return cls(enc, "<|bos|>")
```

### ğŸ—ï¸ æ„å»ºç³»ç»Ÿé›†æˆ

é€šè¿‡ **Maturin** å·¥å…·å°† Rust ä»£ç ç¼–è¯‘ä¸º Python æ‰©å±•ï¼š

```bash
# æ„å»ºå¹¶å®‰è£… Rust æ‰©å±•
uv run maturin develop --release --manifest-path rustbpe/Cargo.toml
```

é…ç½®åœ¨ `pyproject.toml` ä¸­ï¼š
```toml
requires = ["maturin>=1.7,<2.0"]
build-backend = "maturin"

[tool.maturin]
python-source = "python"
module-name = "rustbpe._rustbpe"
features = ["pyo3/extension-module"]
```

### ğŸš€ æ€§èƒ½ä¼˜åŠ¿

ç›¸æ¯”çº¯ Python å®ç°ï¼š

1. **è®­ç»ƒé€Ÿåº¦**: å¹¶è¡Œå¤„ç†å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡
2. **å†…å­˜æ•ˆç‡**: ä¼˜åŒ–çš„æ•°æ®ç»“æ„å‡å°‘å†…å­˜å ç”¨
3. **è¿è¡Œæ€§èƒ½**: ç¼–è¯‘ä¸ºåŸç”Ÿä»£ç ï¼Œæ‰§è¡Œé€Ÿåº¦æ›´å¿«

### ğŸ’¡ è®¾è®¡ç†å¿µ

rustbpe ä½“ç°äº† **"ç®€å•ä½†é«˜æ•ˆ"** çš„è®¾è®¡å“²å­¦ï¼š

- é¿å…è¿‡åº¦å¤æ‚çš„é…ç½®ç³»ç»Ÿ
- ä¸“æ³¨äº BPE è®­ç»ƒçš„æ ¸å¿ƒåŠŸèƒ½
- ä¿æŒä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
- ä¸ç°æœ‰çš„ tiktoken ç”Ÿæ€ç³»ç»Ÿå®Œç¾é›†æˆ

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
rustbpe æ˜¯ç°ä»£ AI å·¥ç¨‹æ ˆä¸­çš„ä¸€ä¸ªä¼˜ç§€ç¤ºä¾‹ï¼Œå®ƒé€šè¿‡é€‰æ‹©åˆé€‚çš„è¯­è¨€ (Rust) å’Œæ¶æ„ (PyO3 + tiktoken)ï¼Œåœ¨æ€§èƒ½ã€æ˜“ç”¨æ€§å’Œç”Ÿæ€ç³»ç»Ÿå…¼å®¹æ€§ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚è¿™ç§è®¾è®¡æ¨¡å¼åœ¨å¤„ç†æ€§èƒ½æ•æ„Ÿçš„ AI åŸºç¡€è®¾æ–½ç»„ä»¶æ—¶è¶Šæ¥è¶Šå¸¸è§ã€‚
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

### ğŸ“Š åœ¨ nanochat ä¸­çš„ä½œç”¨

åœ¨ nanochat çš„è®­ç»ƒæµç¨‹ä¸­ï¼Œrustbpe è´Ÿè´£ï¼š

1. **é¢„å¤„ç†é˜¶æ®µ**: å°†åŸå§‹è®­ç»ƒæ–‡æœ¬è½¬æ¢ä¸º token åºåˆ—
2. **è¯æ±‡è¡¨æ„å»º**: ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜çš„ token è¯æ±‡è¡¨
3. **æ¨ç†æ”¯æŒ**: ç”Ÿæˆçš„è¯æ±‡è¡¨å¯ç”¨äºé«˜æ•ˆçš„æ¨¡å‹æ¨ç†

è¿™ä½¿å¾— nanochat èƒ½å¤Ÿæ‹¥æœ‰è‡ªå®šä¹‰çš„ã€é’ˆå¯¹ç‰¹å®šæ•°æ®ä¼˜åŒ–çš„åˆ†è¯å™¨ï¼Œè€Œä¸æ˜¯ä¾èµ–é€šç”¨çš„é¢„è®­ç»ƒåˆ†è¯å™¨ã€‚

---

### é—®é¢˜ 2
ç”¨æˆ·ï¼šæˆ‘çœ‹åˆ°æ ¸å¿ƒæœ‰ä¸‰ä»½æ–‡ä»¶:Cargo.lock,Cargo.tomlå’Œlib.rs,è¯·è¯¦ç»†ä»‹ç»æ¯ä¸€ä»½æ–‡ä»¶çš„ä½œç”¨.
åŠ©æ‰‹ï¼šè®©æˆ‘è¯¦ç»†ä»‹ç» rustbpe æ–‡ä»¶å¤¹ä¸­è¿™ä¸‰ä»½æ ¸å¿ƒæ–‡ä»¶çš„å…·ä½“ä½œç”¨å’Œå†…å®¹ã€‚

## 1. Cargo.toml - é¡¹ç›®é…ç½®æ–‡ä»¶

`Cargo.toml` æ˜¯ Rust é¡¹ç›®çš„**æ ¸å¿ƒé…ç½®æ–‡ä»¶**ï¼Œå®šä¹‰äº†é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯å’Œä¾èµ–å…³ç³»ï¼š

### ğŸ“¦ é¡¹ç›®åŸºæœ¬ä¿¡æ¯
```toml
[package]
name = "rustbpe"          # é¡¹ç›®åç§°
version = "0.1.0"         # ç‰ˆæœ¬å·
edition = "2024"          # Rust ç‰ˆæœ¬ (ä½¿ç”¨æœ€æ–°çš„ 2024 ç‰ˆæœ¬)
```

### ğŸ”— ä¾èµ–åº“è¯¦ç»†è¯´æ˜

```toml
[dependencies]
dary_heap = "0.3"                    # å…«å‰å †å®ç°ï¼Œç”¨äºä¼˜å…ˆé˜Ÿåˆ—ä¼˜åŒ–
indexmap = "2.2"                     # ä¿æŒæ’å…¥é¡ºåºçš„ HashMap
fancy-regex = "0.16.1"              # æ”¯æŒ Unicode çš„é«˜çº§æ­£åˆ™è¡¨è¾¾å¼
log = "0.4.28"                       # æ—¥å¿—è®°å½•æ¡†æ¶
pyo3 = { version = "0.23.3", features = ["extension-module"] }  # Python ç»‘å®š
pyo3-log = "0.12.4"                  # Rust æ—¥å¿—åˆ° Python logging çš„æ¡¥æ¥
ahash = "0.8.12"                     # é«˜æ€§èƒ½éåŠ å¯†å“ˆå¸Œå‡½æ•°
rayon = "1.11.0"                     # æ•°æ®å¹¶è¡Œè®¡ç®—åº“
compact_str = "0.9.0"                # å†…å­˜ä¼˜åŒ–çš„å­—ç¬¦ä¸²å­˜å‚¨
```

**å…³é”®ä¾èµ–çš„ä½œç”¨**ï¼š
- **pyo3**: æ ¸å¿ƒç»‘å®šï¼Œè®© Rust ä»£ç å¯ä»¥è¢« Python è°ƒç”¨
- **rayon**: å®ç°å¹¶è¡Œå¤„ç†ï¼Œå¤§å¹…æå‡ BPE è®­ç»ƒæ€§èƒ½
- **dary_heap**: ä¼˜åŒ–çš„å †ç»“æ„ï¼Œç”¨äºé«˜æ•ˆæŸ¥æ‰¾æœ€é«˜é¢‘å­—èŠ‚å¯¹
- **fancy-regex**: æ”¯æŒ GPT-4 é£æ ¼çš„å¤æ‚æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
- **ahash**: æ¯” std::collections::HashMap æ›´å¿«çš„å“ˆå¸Œè¡¨

## 2. Cargo.lock - ä¾èµ–é”å®šæ–‡ä»¶

`Cargo.lock` æ˜¯ Rust åŒ…ç®¡ç†å™¨ **è‡ªåŠ¨ç”Ÿæˆ**çš„é”å®šæ–‡ä»¶ï¼Œç¡®ä¿æ„å»ºçš„å¯é‡ç°æ€§ï¼š

### ğŸ¯ æ ¸å¿ƒä½œç”¨

1. **ç‰ˆæœ¬é”å®š**: é”å®šæ‰€æœ‰ä¾èµ–åº“çš„ç¡®åˆ‡ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ä¼ é€’ä¾èµ–
2. **å¯é‡ç°æ„å»º**: ä»»ä½•äººåœ¨ä»»ä½•æœºå™¨ä¸Šæ„å»ºéƒ½èƒ½å¾—åˆ°ç›¸åŒçš„ç»“æœ
3. **å®‰å…¨å®¡è®¡**: è®°å½•æ¯ä¸ªä¾èµ–çš„æ ¡éªŒå’Œ (checksum)ï¼Œé˜²æ­¢ç¯¡æ”¹

### ğŸ“‹ æ–‡ä»¶ç»“æ„åˆ†æ

```toml
# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 4

[[package]]
name = "ahash"                           # ä¾èµ–åŒ…åç§°
version = "0.8.12"                      # ç¡®åˆ‡ç‰ˆæœ¬å·
source = "registry+https://github.com/rust-lang/crates.io-index"  # ä¸‹è½½æº
checksum = "5a15f179cd60c4584b8a8c596927aadc462e27f2ca70c04e0071964a73ba7a75"  # SHA256 æ ¡éªŒå’Œ
dependencies = [                        # è¯¥åŒ…çš„ä¾èµ–
 "cfg-if",
 "getrandom",
 "once_cell",
 "version_check",
 "zerocopy",
]
```

### ğŸ”’ é‡è¦ç‰¹æ€§

- **è‡ªåŠ¨ç”Ÿæˆ**: ä¸åº”æ‰‹åŠ¨ç¼–è¾‘ï¼Œç”± `cargo build` æˆ– `cargo update` è‡ªåŠ¨æ›´æ–°
- **å®Œæ•´ä¾èµ–å›¾**: åŒ…å«æ‰€æœ‰ç›´æ¥å’Œé—´æ¥ä¾èµ–ï¼Œå½¢æˆå®Œæ•´çš„ä¾èµ–æ ‘
- **ä¼ é€’ä¾èµ–è§£æ**: è‡ªåŠ¨å¤„ç†ä¾èµ–å†²çªå’Œç‰ˆæœ¬å…¼å®¹æ€§

### ğŸ’¼ åœ¨å›¢é˜Ÿåä½œä¸­çš„ä½œç”¨

```bash
# æ–°å¼€å‘è€…åŠ å…¥é¡¹ç›®æ—¶
git clone <project>     # è·å–åŒ…å« Cargo.lock çš„ä»£ç 
cargo build            # ä½¿ç”¨é”å®šçš„ç‰ˆæœ¬æ„å»ºï¼Œç¡®ä¿ä¸€è‡´æ€§
```

è¿™ç¡®ä¿äº†å›¢é˜Ÿä¸­æ‰€æœ‰å¼€å‘è€…ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä¾èµ–ç‰ˆæœ¬ï¼Œé¿å…äº†"åœ¨æˆ‘æœºå™¨ä¸Šèƒ½è·‘"çš„é—®é¢˜ã€‚

## 3. lib.rs - æ ¸å¿ƒå®ç°æ–‡ä»¶

`lib.rs` æ˜¯ rustbpe çš„**æ ¸å¿ƒå®ç°æ–‡ä»¶**ï¼ŒåŒ…å«äº†å®Œæ•´çš„ BPE åˆ†è¯å™¨è®­ç»ƒé€»è¾‘ã€‚è®©æˆ‘è¯¦ç»†è§£æå…¶ç»“æ„ï¼š

### ğŸ—ï¸ æ•´ä½“æ¶æ„

```rust
use std::cmp::Ordering;
use std::collections::HashMap as StdHashMap;

use dary_heap::OctonaryHeap;           // å…«å‰å †ä¼˜å…ˆé˜Ÿåˆ—
use fancy_regex::Regex;                // é«˜çº§æ­£åˆ™è¡¨è¾¾å¼
use pyo3::prelude::*;                  // Python ç»‘å®š
use ahash::{AHashMap, AHashSet};       // é«˜æ€§èƒ½å“ˆå¸Œè¡¨
use compact_str::CompactString;        // å†…å­˜ä¼˜åŒ–å­—ç¬¦ä¸²
use rayon::prelude::*;                 // å¹¶è¡Œè®¡ç®—
```

### ğŸ¯ æ ¸å¿ƒæ•°æ®ç»“æ„

#### 1. **Tokenizer ä¸»ç»“æ„**
```rust
#[pyclass]                              // æš´éœ²ç»™ Python çš„ç±»
pub struct Tokenizer {
    pub merges: StdHashMap<Pair, u32>,   // å­—èŠ‚å¯¹åˆ°æ–° token çš„æ˜ å°„
    pub pattern: String,                 // æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    compiled_pattern: Regex,             // é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
}
```

#### 2. **Word å†…éƒ¨ç»“æ„**
```rust
struct Word {
    ids: Vec<u32>,                      // token ID åºåˆ—
}
```

### ğŸ”„ BPE è®­ç»ƒæ ¸å¿ƒç®—æ³•

#### **è®­ç»ƒæµç¨‹** (`train_core_incremental`)

1. **åˆå§‹åŒ–é˜¶æ®µ**:
```rust
// è®¡ç®—åˆå§‹å­—èŠ‚å¯¹é¢‘ç‡ (å¹¶è¡Œ)
let (mut pair_counts, mut where_to_update) = count_pairs_parallel(&words, &counts);

// æ„å»ºä¼˜å…ˆå †
let mut heap = OctonaryHeap::with_capacity(pair_counts.len());
```

2. **è¿­ä»£åˆå¹¶**:
```rust
while merges_done < num_merges {
    let Some(mut top) = heap.pop() else { break };

    // è®°å½•åˆå¹¶è§„åˆ™
    let new_id = 256 + merges_done;
    self.merges.insert(top.pair, new_id);

    // åº”ç”¨åˆå¹¶åˆ°æ‰€æœ‰ç›¸å…³å•è¯
    for &word_idx in &top.pos {
        let changes = words[word_idx].merge_pair(top.pair, new_id);
        // æ›´æ–°é¢‘ç‡è®¡æ•°
    }
}
```

### âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

#### 1. **å¹¶è¡Œå¤„ç†**
```rust
fn count_pairs_parallel(
    words: &[Word],
    counts: &[i32],
) -> (AHashMap<Pair, i32>, AHashMap<Pair, AHashSet<usize>>) {
    words
        .par_iter()                    // å¹¶è¡Œè¿­ä»£
        .enumerate()
        .map(|(i, w)| {
            // æœ¬åœ°è®¡ç®—
        })
        .reduce(|| (AHashMap::new(), AHashMap::new()), |acc, local| {
            // åˆå¹¶ç»“æœ
        })
}
```

#### 2. **å¢é‡æ›´æ–°**
```rust
// é¿å…é‡æ–°è®¡ç®—æ‰€æœ‰é¢‘ç‡ï¼Œåªæ›´æ–°å—å½±å“çš„éƒ¨åˆ†
let changes = words[word_idx].merge_pair(top.pair, new_id);
for (pair, delta) in changes {
    *pair_counts.entry(pair).or_default() += delta_total;
}
```

#### 3. **å†…å­˜ä¼˜åŒ–**
```rust
// ä½¿ç”¨ CompactString å‡å°‘å°å­—ç¬¦ä¸²çš„å†…å­˜å¼€é”€
let mut counts: AHashMap<CompactString, i32> = AHashMap::new();

// ä½¿ç”¨ AHash æ›¿ä»£æ ‡å‡† HashMap æå‡æ€§èƒ½
use ahash::{AHashMap, AHashSet};
```

### ğŸ Python æ¥å£è®¾è®¡

#### **è®­ç»ƒæ¥å£** (`train_from_iterator`)
```rust
#[pyo3(signature = (iterator, vocab_size, buffer_size=8192, pattern=None))]
#[pyo3(text_signature = "(self, iterator, vocab_size, buffer_size=8192, pattern=None)")]
pub fn train_from_iterator(
    &mut self,
    py: pyo3::Python<'_>,
    iterator: &pyo3::Bound<'_, pyo3::PyAny>,  // Python è¿­ä»£å™¨
    vocab_size: u32,
    buffer_size: usize,
    pattern: Option<String>,
) -> PyResult<()>
```

**æµå¼å¤„ç†è®¾è®¡**:
- ç¼“å†²åŒºæœºåˆ¶å‡å°‘ Python-Rust è·¨ç•Œå¼€é”€
- æ‰¹é‡å¤„ç†æå‡å¹¶è¡Œæ•ˆç‡
- GIL (å…¨å±€è§£é‡Šå™¨é”) ç®¡ç†

#### **æ¨ç†æ¥å£** (`encode`)
```rust
pub fn encode(&self, text: &str) -> Vec<u32> {
    // æ­£åˆ™åˆ†è¯
    for m in self.compiled_pattern.find_iter(text) {
        // å­—èŠ‚çº§ç¼–ç 
        let mut ids: Vec<u32> = chunk.bytes().map(|b| b as u32).collect();

        // åº”ç”¨åˆå¹¶è§„åˆ™
        while ids.len() >= 2 {
            // æŸ¥æ‰¾æœ€ä½³åˆå¹¶å¯¹
            // æ‰§è¡Œåˆå¹¶
        }
    }
}
```

### ğŸ”§ æ¨¡å—åˆå§‹åŒ–

```rust
#[pymodule]
fn rustbpe(m: &Bound<'_, PyModule>) -> PyResult<()> {
    pyo3_log::init();                   // é›†æˆ Rust æ—¥å¿—åˆ° Python logging
    m.add_class::<Tokenizer>()?;        // æ³¨å†Œ Tokenizer ç±»
    Ok(())
}
```

### ğŸ’¡ è®¾è®¡äº®ç‚¹

1. **é›¶æ‹·è´è®¾è®¡**: å°½å¯èƒ½é¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶
2. **æ‡’åŠ è½½**: æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘ï¼Œé¿å…é‡å¤ç¼–è¯‘
3. **å†…å­˜å±€éƒ¨æ€§**: ä¼˜åŒ–æ•°æ®ç»“æ„å¸ƒå±€æå‡ç¼“å­˜æ•ˆç‡
4. **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯ä¼ æ’­å’Œå¤„ç†æœºåˆ¶

### ğŸ“Š ä¸‰æ–‡ä»¶çš„åä½œå…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ç¼–è¯‘é…ç½®    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cargo.toml    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚   Cargo.lock    â”‚
â”‚ (é¡¹ç›®é…ç½®)       â”‚                â”‚ (ç‰ˆæœ¬é”å®š)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                   â”‚
         â”‚ ä¾èµ–å£°æ˜                            â”‚ é”å®šç‰ˆæœ¬
         â†“                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    lib.rs                               â”‚
â”‚                  (æ ¸å¿ƒå®ç°)                              â”‚
â”‚  â€¢ ä½¿ç”¨ Cargo.toml ä¸­å£°æ˜çš„ä¾èµ–                          â”‚
â”‚  â€¢ æŒ‰ Cargo.lock é”å®šçš„ç‰ˆæœ¬æ„å»º                          â”‚
â”‚  â€¢ å®ç° BPE ç®—æ³•å’Œ Python ç»‘å®š                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ æ€»ç»“

è¿™ä¸‰ä»½æ–‡ä»¶æ„æˆäº†ä¸€ä¸ªå®Œæ•´çš„ Rust é¡¹ç›®ç»“æ„ï¼š

1. **Cargo.toml**: **è®¾è®¡è“å›¾** - å®šä¹‰é¡¹ç›®éœ€æ±‚å’Œä¾èµ–å…³ç³»
2. **Cargo.lock**: **æ‰§è¡Œå¥‘çº¦** - ç¡®ä¿æ„å»ºçš„å¯é‡ç°æ€§å’Œä¸€è‡´æ€§
3. **lib.rs**: **æ ¸å¿ƒå®ç°** - å®ç°å…·ä½“çš„ä¸šåŠ¡é€»è¾‘å’Œç®—æ³•

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
è¿™ç§ä¸‰å±‚æ¶æ„è®¾è®¡ä½“ç°äº†ç°ä»£è½¯ä»¶å·¥ç¨‹çš„æœ€ä½³å®è·µï¼šé…ç½®ä¸å®ç°åˆ†ç¦»ã€ç‰ˆæœ¬æ§åˆ¶å¯é‡ç°ã€æ¨¡å—åŒ–è®¾è®¡ã€‚rustbpe é€šè¿‡è¿™ç§ç»“æ„å®ç°äº†é«˜æ€§èƒ½çš„ BPE è®­ç»ƒå™¨ï¼ŒåŒæ—¶ä¿æŒäº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå›¢é˜Ÿåä½œçš„å‹å¥½æ€§ã€‚
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

åœ¨ nanochat é¡¹ç›®ä¸­ï¼Œrustbpe å±•ç¤ºäº†å¦‚ä½•ç”¨ Rust é‡å†™ Python ä¸­çš„æ€§èƒ½ç“¶é¢ˆä»£ç ï¼Œé€šè¿‡ PyO3 æ— ç¼é›†æˆï¼Œæ—¢ä¿æŒäº† Python çš„æ˜“ç”¨æ€§ï¼Œåˆè·å¾—äº† Rust çš„é«˜æ€§èƒ½ã€‚è¿™ç§æ¨¡å¼åœ¨ AI å·¥ç¨‹æ ˆä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚

---

### é—®é¢˜ 3
ç”¨æˆ·ï¼šrustbpeå®ç°çš„æ˜¯tokençš„è®­ç»ƒè¿˜æ˜¯é¢„æµ‹?
åŠ©æ‰‹ï¼šæ ¹æ®æˆ‘åˆšæ‰å¯¹ rustbpe ä»£ç çš„è¯¦ç»†åˆ†æï¼Œ**rustbpe ä¸»è¦å®ç°çš„æ˜¯ token çš„è®­ç»ƒï¼Œè€Œä¸æ˜¯é¢„æµ‹ï¼ˆæ¨ç†ï¼‰**ã€‚

è®©æˆ‘å…·ä½“è§£é‡Šä¸€ä¸‹ï¼š

## ğŸ¯ rustbpe çš„æ ¸å¿ƒåŠŸèƒ½ï¼šè®­ç»ƒ

rustbpe å®ç°çš„æ˜¯ **BPE (Byte Pair Encoding) åˆ†è¯å™¨çš„è®­ç»ƒè¿‡ç¨‹**ï¼š

### è®­ç»ƒåŠŸèƒ½è¯¦è§£

#### 1. **è¯æ±‡è¡¨å­¦ä¹ **
```rust
// lib.rs:164-256 - æ ¸å¿ƒè®­ç»ƒç®—æ³•
fn train_core_incremental(&mut self, mut words: Vec<Word>, counts: Vec<i32>, vocab_size: u32) {
    // ä»åŸå§‹æ–‡æœ¬ä¸­å­¦ä¹ å­—èŠ‚å¯¹åˆå¹¶è§„åˆ™
    // è¿­ä»£åœ°å°†æœ€é«˜é¢‘çš„å­—èŠ‚å¯¹åˆå¹¶ä¸ºæ–°çš„ token
    // æœ€ç»ˆæ„å»ºå‡ºå®Œæ•´çš„è¯æ±‡è¡¨
}
```

#### 2. **åˆå¹¶è§„åˆ™å­¦ä¹ **
```rust
// lib.rs:210-212 - è®°å½•åˆå¹¶è§„åˆ™
let new_id = 256 + merges_done;
self.merges.insert(top.pair, new_id);  // å°†å­—èŠ‚å¯¹æ˜ å°„åˆ°æ–° token ID
```

#### 3. **é¢‘ç‡ç»Ÿè®¡å’Œä¼˜åŒ–**
```rust
// lib.rs:125-155 - å¹¶è¡Œç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡
fn count_pairs_parallel(...) -> (AHashMap<Pair, i32>, AHashMap<Pair, AHashSet<usize>>) {
    // ä½¿ç”¨ Rayon å¹¶è¡Œè®¡ç®—æ‰€æœ‰å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡
    // é€‰æ‹©æœ€é«˜é¢‘çš„å­—èŠ‚å¯¹è¿›è¡Œåˆå¹¶
}
```

## ğŸ” æ¨ç†ï¼ˆé¢„æµ‹ï¼‰åŠŸèƒ½çš„å®ç°

rustbpe **ç¡®å®åŒ…å«æ¨ç†åŠŸèƒ½**ï¼Œä½†è¿™æ˜¯ä¸ºäº†**è®­ç»ƒåæµ‹è¯•å’ŒéªŒè¯**ï¼Œè€Œä¸æ˜¯ä¸»è¦ç”¨é€”ï¼š

### æ¨ç†å®ç°
```rust
// lib.rs:428-467 - ç¼–ç æ–‡æœ¬ä¸º token åºåˆ—
pub fn encode(&self, text: &str) -> Vec<u32> {
    // ä½¿ç”¨å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™å°†æ–°æ–‡æœ¬ç¼–ç ä¸º token ID
    // è¿™ä¸ªåŠŸèƒ½ä¸»è¦ç”¨äºéªŒè¯è®­ç»ƒç»“æœ
}
```

## ğŸ—ï¸ å®Œæ•´çš„åˆ†è¯å™¨ç”Ÿæ€ç³»ç»Ÿ

åœ¨ nanochat é¡¹ç›®ä¸­ï¼Œåˆ†è¯å™¨çš„å®Œæ•´å·¥ä½œæµç¨‹æ˜¯è¿™æ ·çš„ï¼š

```python
# nanochat/tokenizer.py - å®Œæ•´æµç¨‹
class RustBPETokenizer:
    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        # 1ï¸âƒ£ è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨ rustbpe
        tokenizer = rustbpe.Tokenizer()
        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)

        # 2ï¸âƒ£ å¯¼å‡ºè¯æ±‡è¡¨
        mergeable_ranks = {bytes(k): v for k, v in tokenizer.get_mergeable_ranks()}

        # 3ï¸âƒ£ æ„å»ºæ¨ç†å¼•æ“ï¼šä½¿ç”¨ tiktoken
        enc = tiktoken.Encoding(
            name="rustbpe",
            pat_str=pattern,
            mergeable_ranks=mergeable_ranks,
            special_tokens=special_tokens,
        )

        return cls(enc, "<|bos|>")  # è¿”å›ä½¿ç”¨ tiktoken çš„æ¨ç†å™¨
```

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

### 1. **è®­ç»ƒ vs æ¨ç†çš„æ€§èƒ½éœ€æ±‚ä¸åŒ**
- **è®­ç»ƒ**: éœ€è¦å¤„ç†å¤§é‡æ•°æ®ï¼Œå¤æ‚ç®—æ³•ï¼Œé€‚åˆ Rust çš„é«˜æ€§èƒ½
- **æ¨ç†**: éœ€è¦é«˜é¢‘ç‡è°ƒç”¨ï¼Œè½»é‡çº§ï¼Œé€‚åˆ tiktoken çš„ä¼˜åŒ–å®ç°

### 2. **ç”Ÿæ€å…¼å®¹æ€§**
- **tiktoken**: OpenAI å®˜æ–¹æ¨ç†åº“ï¼Œå¹¿æ³›ä½¿ç”¨ï¼Œæ€§èƒ½ä¼˜å¼‚
- **rustbpe**: å¡«è¡¥ tiktoken æ— æ³•è®­ç»ƒçš„ç©ºç™½

### 3. **å®é™…ä½¿ç”¨æ¨¡å¼**
```bash
# è®­ç»ƒé˜¶æ®µï¼šä¸€æ¬¡æ€§è¿è¡Œï¼Œä½¿ç”¨ rustbpe
python -m scripts.tok_train --max_chars=2000000000

# æ¨ç†é˜¶æ®µï¼šé¢‘ç¹è°ƒç”¨ï¼Œä½¿ç”¨ tiktoken
python -m scripts.chat_cli  # å†…éƒ¨ä½¿ç”¨ tiktoken è¿›è¡Œç¼–ç 
```

## ğŸ“Š åŠŸèƒ½å¯¹æ¯”æ€»ç»“

| åŠŸèƒ½ | rustbpe | tiktoken |
|------|---------|----------|
| **BPE è®­ç»ƒ** | âœ… ä¸»è¦åŠŸèƒ½ | âŒ ä¸æ”¯æŒ |
| **æ–‡æœ¬ç¼–ç ** | âœ… æ”¯æŒï¼ˆæµ‹è¯•ç”¨ï¼‰ | âœ… ä¸»è¦åŠŸèƒ½ |
| **æ–‡æœ¬è§£ç ** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| **æ€§èƒ½ä¼˜åŒ–** | è®­ç»ƒæ€§èƒ½ä¼˜åŒ– | æ¨ç†æ€§èƒ½ä¼˜åŒ– |
| **Python é›†æˆ** | PyO3 ç»‘å®š | åŸç”Ÿ Python |

## ğŸ¯ ç»“è®º

**rustbpe ä¸“æ³¨äºè®­ç»ƒ**ï¼Œå®ƒçš„ä¸»è¦ä»·å€¼åœ¨äºï¼š

1. **å¡«è¡¥ç”Ÿæ€ç©ºç™½**: æä¾›äº† tiktoken ç¼ºå¤±çš„è®­ç»ƒåŠŸèƒ½
2. **é«˜æ€§èƒ½è®­ç»ƒ**: ä½¿ç”¨ Rust å®ç°é«˜æ•ˆçš„å¹¶è¡Œ BPE è®­ç»ƒ
3. **æ— ç¼é›†æˆ**: è®­ç»ƒç»“æœå¯ç›´æ¥ç”¨äº tiktoken æ¨ç†

è¿™ç§"**è®­ç»ƒç”¨ Rustï¼Œæ¨ç†ç”¨ tiktoken**"çš„è®¾è®¡æ¨¡å¼ï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§å·¥å…·çš„ä¼˜åŠ¿ï¼Œæ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„æ¶æ„é€‰æ‹©ã€‚

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
åœ¨ç°ä»£ AI å·¥ç¨‹æ ˆä¸­ï¼Œè¿™ç§åˆ†å·¥åä½œçš„æ¨¡å¼è¶Šæ¥è¶Šå¸¸è§ï¼šä½¿ç”¨é«˜æ€§èƒ½è¯­è¨€å®ç°è®¡ç®—å¯†é›†å‹çš„è®­ç»ƒç®—æ³•ï¼ŒåŒæ—¶ä¿æŒä¸ç°æœ‰æ¨ç†ç”Ÿæ€çš„å…¼å®¹æ€§ã€‚rustbpe å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ï¼Œå®ƒä¸é‡å¤é€ è½®å­ï¼Œè€Œæ˜¯ä¸“æ³¨äºè§£å†³ç‰¹å®šçš„æŠ€æœ¯ç—›ç‚¹ã€‚
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

---

### é—®é¢˜ 4
ç”¨æˆ·ï¼šrustbpe ä¸»è¦å®ç°çš„æ˜¯ token çš„è®­ç»ƒï¼Œè€Œä¸æ˜¯é¢„æµ‹ï¼ˆæ¨ç†ï¼‰ã€‚

------

è¯·ç»™å‡ºä½¿ç”¨å…¶è¿›è¡Œè®­ç»ƒçš„å…·ä½“ä»£ç ç¤ºä¾‹.ç»™å‡ºä»£ç ç¤ºä¾‹å³å¯,ä¸ç”¨äº²è‡ªæ‰§è¡Œ!
åŠ©æ‰‹ï¼šå¥½çš„ï¼Œæˆ‘æ¥æä¾›ä½¿ç”¨ rustbpe è¿›è¡Œè®­ç»ƒçš„å…·ä½“ä»£ç ç¤ºä¾‹ã€‚

## ğŸ“ rustbpe è®­ç»ƒä»£ç ç¤ºä¾‹

### 1. **ç›´æ¥ä½¿ç”¨ rustbpe è¿›è¡Œè®­ç»ƒ**

```python
import rustbpe

# ç¤ºä¾‹è®­ç»ƒæ•°æ®
training_texts = [
    "Hello world! This is a test of the BPE tokenizer.",
    "Machine learning models need good tokenization.",
    "Tokenization is an important preprocessing step.",
    "Byte Pair Encoding splits text into meaningful tokens.",
    "Natural language processing relies on tokenizers.",
    "Deep learning models require efficient tokenizers.",
    "Training custom tokenizers improves model performance.",
    "Modern AI systems use subword tokenization methods."
]

# åˆ›å»ºè®­ç»ƒè¿­ä»£å™¨
def text_iterator():
    for text in training_texts:
        yield text

# åˆ›å»ºå¹¶è®­ç»ƒåˆ†è¯å™¨
tokenizer = rustbpe.Tokenizer()
vocab_size = 1000  # ç›®æ ‡è¯æ±‡è¡¨å¤§å°

# å¼€å§‹è®­ç»ƒ
tokenizer.train_from_iterator(
    iterator=text_iterator(),
    vocab_size=vocab_size,
    buffer_size=1024,  # ç¼“å†²åŒºå¤§å°
    pattern=r"'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"
)

print(f"è®­ç»ƒå®Œæˆ! è¯æ±‡è¡¨å¤§å°: {len(tokenizer.get_mergeable_ranks())}")

# æµ‹è¯•ç¼–ç åŠŸèƒ½
test_text = "Hello, this is a test!"
encoded = tokenizer.encode(test_text)
print(f"æµ‹è¯•æ–‡æœ¬: '{test_text}'")
print(f"ç¼–ç ç»“æœ: {encoded}")
```

### 2. **Nanochat ä¸­çš„å®Œæ•´è®­ç»ƒæµç¨‹**

```python
# scripts/tok_train.py (ç®€åŒ–ç‰ˆ)
import rustbpe
import tiktoken
import os
from nanochat.tokenizer import SPECIAL_TOKENS, SPLIT_PATTERN

def main():
    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    def text_data_stream():
        # è¿™é‡Œå¯ä»¥æ˜¯æ–‡ä»¶è¯»å–ã€æ•°æ®åº“æŸ¥è¯¢ç­‰
        data_files = [
            "data/train_text1.txt",
            "data/train_text2.txt",
            "data/web_crawl_data.txt"
        ]

        for file_path in data_files:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        yield line.strip()

    # 2. åˆ›å»º rustbpe è®­ç»ƒå™¨
    tokenizer = rustbpe.Tokenizer()

    # 3. è®­ç»ƒé…ç½®
    target_vocab_size = 32768  # 32K è¯æ±‡è¡¨
    vocab_size_no_special = target_vocab_size - len(SPECIAL_TOKENS)

    print(f"å¼€å§‹è®­ç»ƒ BPE åˆ†è¯å™¨...")
    print(f"ç›®æ ‡è¯æ±‡è¡¨å¤§å°: {target_vocab_size}")
    print(f"éç‰¹æ®Š token å¤§å°: {vocab_size_no_special}")

    # 4. æ‰§è¡Œè®­ç»ƒ
    tokenizer.train_from_iterator(
        iterator=text_data_stream(),
        vocab_size=vocab_size_no_special,
        buffer_size=8192,  # 8K ç¼“å†²åŒº
        pattern=SPLIT_PATTERN
    )

    # 5. è·å–è®­ç»ƒç»“æœ
    pattern = tokenizer.get_pattern()
    mergeable_ranks_list = tokenizer.get_mergeable_ranks()
    mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}

    # 6. æ„å»º tiktoken ç¼–ç å™¨ç”¨äºæ¨ç†
    tokens_offset = len(mergeable_ranks)
    special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}

    enc = tiktoken.Encoding(
        name="rustbpe_custom",
        pat_str=pattern,
        mergeable_ranks=mergeable_ranks,
        special_tokens=special_tokens,
    )

    # 7. ä¿å­˜è®­ç»ƒç»“æœ
    tokenizer_dir = "tokenizer"
    os.makedirs(tokenizer_dir, exist_ok=True)

    # ä¿å­˜ tiktoken ç¼–ç å™¨
    import pickle
    with open(os.path.join(tokenizer_dir, "tokenizer.pkl"), "wb") as f:
        pickle.dump(enc, f)

    print(f"åˆ†è¯å™¨è®­ç»ƒå®Œæˆå¹¶ä¿å­˜åˆ°: {tokenizer_dir}")
    print(f"å®é™…è¯æ±‡è¡¨å¤§å°: {enc.n_vocab}")

    # 8. éªŒè¯è®­ç»ƒç»“æœ
    test_samples = [
        "Hello, world!",
        "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ",
        "Byte Pair Encoding tokenization",
        "æ·±åº¦å­¦ä¹ å’Œäººå·¥æ™ºèƒ½"
    ]

    print("\nè®­ç»ƒç»“æœéªŒè¯:")
    for text in test_samples:
        encoded = enc.encode(text)
        decoded = enc.decode(encoded)
        print(f"åŸæ–‡: {text}")
        print(f"ç¼–ç : {encoded}")
        print(f"è§£ç : {decoded}")
        print("-" * 50)

if __name__ == "__main__":
    main()
```

### 3. **å¤§è§„æ¨¡æ•°æ®è®­ç»ƒç¤ºä¾‹**

```python
# å¤§è§„æ¨¡æ•°æ®æµå¼è®­ç»ƒ
import rustbpe
import glob
import time
from pathlib import Path

class LargeScaleTrainer:
    def __init__(self, vocab_size=65536):
        self.vocab_size = vocab_size
        self.tokenizer = rustbpe.Tokenizer()

    def create_data_iterator(self, data_dir_pattern):
        """åˆ›å»ºå¤§è§„æ¨¡æ•°æ®è¿­ä»£å™¨"""
        files = glob.glob(data_dir_pattern)
        total_files = len(files)

        print(f"å‘ç° {total_files} ä¸ªæ•°æ®æ–‡ä»¶")

        for file_idx, file_path in enumerate(files):
            if file_idx % 100 == 0:
                print(f"å¤„ç†æ–‡ä»¶ {file_idx}/{total_files}: {Path(file_path).name}")

            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    for line_num, line in enumerate(f, 1):
                        if line_num % 10000 == 0:
                            print(f"  æ–‡ä»¶ {file_idx+1}: å¤„ç†äº† {line_num} è¡Œ")

                        text = line.strip()
                        if text and len(text) > 10:  # è¿‡æ»¤çŸ­æ–‡æœ¬
                            yield text
            except Exception as e:
                print(f"  è·³è¿‡æ–‡ä»¶ {file_path} (é”™è¯¯: {e})")
                continue

    def train(self, data_dir_pattern, checkpoint_dir="checkpoints"):
        """æ‰§è¡Œå¤§è§„æ¨¡è®­ç»ƒ"""
        print("å¼€å§‹å¤§è§„æ¨¡ BPE è®­ç»ƒ...")
        start_time = time.time()

        # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
        data_iter = self.create_data_iterator(data_dir_pattern)

        # æ‰§è¡Œè®­ç»ƒ
        self.tokenizer.train_from_iterator(
            iterator=data_iter,
            vocab_size=self.vocab_size,
            buffer_size=16384,  # 16K ç¼“å†²åŒºä»¥æå‡æ€§èƒ½
            pattern=r"'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"
        )

        end_time = time.time()
        training_time = end_time - start_time

        print(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f} ç§’")
        print(f"æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.get_mergeable_ranks())}")

        return self.tokenizer

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    trainer = LargeScaleTrainer(vocab_size=50000)

    # å‡è®¾æœ‰ä¸€ä¸ªåŒ…å«å¤§é‡æ–‡æœ¬æ–‡ä»¶çš„æ•°æ®é›†
    data_pattern = "/path/to/training/data/*.txt"

    # å¼€å§‹è®­ç»ƒ
    trained_tokenizer = trainer.train(data_pattern)

    # ä¿å­˜è®­ç»ƒç»“æœ
    mergeable_ranks = trained_tokenizer.get_mergeable_ranks()
    print(f"è®­ç»ƒå®Œæˆï¼Œç”Ÿæˆäº† {len(mergeable_ranks)} ä¸ªè¯æ±‡")
```

### 4. **è‡ªå®šä¹‰è®­ç»ƒå‚æ•°ç¤ºä¾‹**

```python
import rustbpe

# è‡ªå®šä¹‰é…ç½®è®­ç»ƒ
class CustomTrainer:
    def __init__(self):
        self.tokenizer = rustbpe.Tokenizer()

    def train_with_custom_config(self, text_data, config):
        """ä½¿ç”¨è‡ªå®šä¹‰é…ç½®è®­ç»ƒ"""

        # è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        custom_pattern = config.get('pattern',
            r"'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"
        )

        # è®­ç»ƒå‚æ•°
        vocab_size = config.get('vocab_size', 10000)
        buffer_size = config.get('buffer_size', 4096)

        print(f"å¼€å§‹è®­ç»ƒï¼Œé…ç½®:")
        print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"  ç¼“å†²åŒºå¤§å°: {buffer_size}")
        print(f"  è‡ªå®šä¹‰æ¨¡å¼: {custom_pattern[:100]}...")

        # æ‰§è¡Œè®­ç»ƒ
        self.tokenizer.train_from_iterator(
            iterator=text_data,
            vocab_size=vocab_size,
            buffer_size=buffer_size,
            pattern=custom_pattern
        )

        return self.tokenizer

# ä½¿ç”¨ç¤ºä¾‹
trainer = CustomTrainer()

# è‡ªå®šä¹‰è®­ç»ƒé…ç½®
training_config = {
    'vocab_size': 8192,      # 8K è¯æ±‡è¡¨
    'buffer_size': 2048,     # 2K ç¼“å†²åŒº
    # 'pattern': custom_regex  # å¯é€‰çš„è‡ªå®šä¹‰æ¨¡å¼
}

# å‡†å¤‡è®­ç»ƒæ•°æ®
sample_data = [
    line for line in open("sample_text.txt", encoding='utf-8')
    if len(line.strip()) > 5
][:1000]  # å–å‰1000è¡Œä½œä¸ºç¤ºä¾‹

# å¼€å§‹è®­ç»ƒ
trained_tokenizer = trainer.train_with_custom_config(sample_data, training_config)

# éªŒè¯ç»“æœ
test_text = "This is a test of our custom trained tokenizer."
encoded = trained_tokenizer.encode(test_text)
print(f"ç¼–ç ç»“æœ: {encoded}")
```

è¿™äº›ä»£ç ç¤ºä¾‹å±•ç¤ºäº† rustbpe è®­ç»ƒåŠŸèƒ½çš„ä¸åŒä½¿ç”¨åœºæ™¯ï¼Œä»ç®€å•çš„ç¤ºä¾‹åˆ°å¤§è§„æ¨¡æ•°æ®è®­ç»ƒï¼Œå†åˆ°è‡ªå®šä¹‰é…ç½®è®­ç»ƒã€‚æ ¸å¿ƒéƒ½æ˜¯ä½¿ç”¨ `train_from_iterator` æ–¹æ³•è¿›è¡Œ BPE åˆ†è¯å™¨çš„è®­ç»ƒã€‚

---

### é—®é¢˜ 5
ç”¨æˆ·ï¼šåŸºäºrustç¼–å†™çš„tokenè®­ç»ƒä»£ç å¯ä»¥è¢«pythonè°ƒç”¨?
åŠ©æ‰‹ï¼šæ˜¯çš„ï¼**åŸºäº Rust ç¼–å†™çš„ token è®­ç»ƒä»£ç å®Œå…¨å¯ä»¥è¢« Python è°ƒç”¨**ï¼Œè¿™æ­£æ˜¯ rustbpe çš„æ ¸å¿ƒä»·å€¼æ‰€åœ¨ã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šè¿™æ˜¯å¦‚ä½•å®ç°çš„ï¼š

## ğŸ”— æŠ€æœ¯åŸç†ï¼šPyO3 ç»‘å®š

### 1. **PyO3 æ¡†æ¶çš„ä½œç”¨**

```rust
// rustbpe/src/lib.rs - PyO3 ç»‘å®šå®ç°
use pyo3::prelude::*;

#[pyclass]  // å°† Rust ç»“æ„ä½“æš´éœ²ä¸º Python ç±»
pub struct Tokenizer {
    pub merges: StdHashMap<Pair, u32>,
    pub pattern: String,
    compiled_pattern: Regex,
}

#[pymethods]  // å°† Rust æ–¹æ³•æš´éœ²ä¸º Python æ–¹æ³•
impl Tokenizer {
    #[new]  // Python æ„é€ å‡½æ•°
    pub fn new() -> Self { ... }

    #[pyo3(signature = (iterator, vocab_size, buffer_size=8192, pattern=None))]
    pub fn train_from_iterator(
        &mut self,
        py: pyo3::Python<'_>,
        iterator: &pyo3::Bound<'_, pyo3::PyAny>,  # Python å¯¹è±¡
        vocab_size: u32,
        buffer_size: usize,
        pattern: Option<String>,
    ) -> PyResult<()> { ... }
}
```

### 2. **æ¨¡å—åˆå§‹åŒ–**
```rust
#[pymodule]
fn rustbpe(m: &Bound<'_, PyModule>) -> PyResult<()> {
    pyo3_log::init();  // é›†æˆ Rust æ—¥å¿—åˆ° Python
    m.add_class::<Tokenizer>()?;  # æ³¨å†Œ Tokenizer ç±»
    Ok(())
}
```

## ğŸ—ï¸ æ„å»ºå’Œé›†æˆæµç¨‹

### 1. **Maturin æ„å»ºç³»ç»Ÿ**

åœ¨ `pyproject.toml` ä¸­é…ç½®ï¼š
```toml
requires = ["maturin>=1.7,<2.0"]
build-backend = "maturin"

[tool.maturin]
python-source = "python"
module-name = "rustbpe._rustbpe"
features = ["pyo3/extension-module"]
```

### 2. **æ„å»ºå‘½ä»¤**
```bash
# å°† Rust ä»£ç ç¼–è¯‘ä¸º Python æ‰©å±•æ¨¡å—
uv run maturin develop --release --manifest-path rustbpe/Cargo.toml
```

è¿™ä¸ªå‘½ä»¤ä¼šï¼š
1. ç¼–è¯‘ Rust ä»£ç ä¸ºåŸç”Ÿåº“
2. åˆ›å»º Python åŒ…è£…å™¨
3. å®‰è£…åˆ° Python ç¯å¢ƒä¸­
4. ä½¿å¾— `import rustbpe` æˆä¸ºå¯èƒ½

## ğŸ Python è°ƒç”¨ç¤ºä¾‹

### 1. **åŸºæœ¬è°ƒç”¨**
```python
# ç›´æ¥åœ¨ Python ä¸­ä½¿ç”¨ Rust å®ç°çš„ Tokenizer
import rustbpe

# åˆ›å»º Rust å®ç°çš„ Tokenizer å®ä¾‹
tokenizer = rustbpe.Tokenizer()

# è°ƒç”¨ Rust å®ç°çš„è®­ç»ƒæ–¹æ³•
def data_stream():
    for i in range(1000):
        yield f"è¿™æ˜¯ç¬¬ {i} æ¡è®­ç»ƒæ•°æ®"

tokenizer.train_from_iterator(
    iterator=data_stream(),
    vocab_size=5000,
    buffer_size=1024
)
```

### 2. **æ•°æ®ç±»å‹è½¬æ¢**

**Python â†’ Rust**:
```rust
// Python è¿­ä»£å™¨è‡ªåŠ¨è½¬æ¢ä¸º Rust å¯å¤„ç†çš„æ ¼å¼
iterator: &pyo3::Bound<'_, pyo3::PyAny>  // Python ä»»æ„å¯¹è±¡
py: pyo3::Python<'_>                      // Python GIL ä¸Šä¸‹æ–‡
```

**Rust â†’ Python**:
```rust
// Rust è¿”å›å€¼è‡ªåŠ¨è½¬æ¢ä¸º Python å¯¹è±¡
pub fn encode(&self, text: &str) -> Vec<u32>  // â†’ Python list[int]
pub fn get_mergeable_ranks(&self) -> Vec<(Vec<u8>, u32)>  // â†’ Python list[tuple[bytes, int]]
```

### 3. **é”™è¯¯å¤„ç†**
```rust
// Rust é”™è¯¯è‡ªåŠ¨è½¬æ¢ä¸º Python å¼‚å¸¸
pub fn train_from_iterator(...) -> PyResult<()> {
    // Rust Result<T, PyErr> è‡ªåŠ¨æ˜ å°„ä¸º Python å¼‚å¸¸
    let regex = Regex::new(&pattern_str)
        .map_err(|e| pyo3::exceptions::PyValueError::new_err(
            format!("Invalid regex pattern: {}", e)
        ))?;
}
```

## âš¡ æ€§èƒ½ä¼˜åŠ¿åˆ†æ

### 1. **è®¡ç®—å¯†é›†å‹ä»»åŠ¡åŠ é€Ÿ**
```python
# Python çº¯å®ç° vs Rust å®ç°çš„æ€§èƒ½å¯¹æ¯”

# Python ç‰ˆæœ¬ (æ…¢)
def python_bpe_train(texts, vocab_size):
    # çº¯ Python å®ç°çš„ BPE è®­ç»ƒ
    # å¤„ç† 1M æ–‡æœ¬å¯èƒ½éœ€è¦æ•°å°æ—¶
    pass

# Rust ç‰ˆæœ¬ (å¿«)
import rustbpe
def rust_bpe_train(texts, vocab_size):
    tokenizer = rustbpe.Tokenizer()
    tokenizer.train_from_iterator(texts, vocab_size)
    # å¤„ç† 1M æ–‡æœ¬å¯èƒ½åªéœ€è¦å‡ åˆ†é’Ÿ
```

### 2. **å¹¶è¡Œå¤„ç†ä¼˜åŠ¿**
```rust
// Rust ä»£ç ä¸­çš„å¹¶è¡Œå¤„ç†
words.par_iter()  // Rayon å¹¶è¡Œè¿­ä»£
    .map(|w| { /* å¤„ç†å•ä¸ªå•è¯ */ })
    .reduce(|| /* åˆå§‹å€¼ */, |acc, local| { /* åˆå¹¶ç»“æœ */ })
```

Rust ä»£ç è‡ªåŠ¨åˆ©ç”¨å¤šæ ¸ CPUï¼Œè€Œ Python å— GIL é™åˆ¶ã€‚

## ğŸŒ‰ è·¨è¯­è¨€äº¤äº’çš„å¤æ‚æ€§

### 1. **GIL (å…¨å±€è§£é‡Šå™¨é”) ç®¡ç†**
```rust
// åœ¨é•¿æ—¶é—´è®¡ç®—ä¸­é‡Šæ”¾ Python GIL
let local: AHashMap<CompactString, i32> = py.allow_threads(|| {
    // è¿™é‡Œ Rust ä»£ç å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œä¸é˜»å¡ Python
    buf.par_iter()
        .map(|s| { /* CPU å¯†é›†å‹è®¡ç®— */ })
        .reduce(...)
});
```

### 2. **å†…å­˜ç®¡ç†**
```rust
// PyO3 è‡ªåŠ¨å¤„ç† Python å’Œ Rust ä¹‹é—´çš„å†…å­˜ç®¡ç†
// Rust çš„æ‰€æœ‰æƒç³»ç»Ÿç¡®ä¿å†…å­˜å®‰å…¨
// Python çš„åƒåœ¾å›æ”¶ä¸ Rust çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†åè°ƒå·¥ä½œ
```

### 3. **ç±»å‹æ˜ å°„**
| Rust ç±»å‹ | Python ç±»å‹ | è¯´æ˜ |
|-----------|-------------|------|
| `String` | `str` | å­—ç¬¦ä¸² |
| `Vec<T>` | `list[T]` | æ•°ç»„/åˆ—è¡¨ |
| `HashMap<K,V>` | `dict[K,V]` | å­—å…¸ |
| `Option<T>` | `Optional[T]` | å¯é€‰å€¼ |
| `Result<T,E>` | `T` æˆ–å¼‚å¸¸ | é”™è¯¯å¤„ç† |

## ğŸ“Š å®é™…æ€§èƒ½å¯¹æ¯”

æ ¹æ® nanochat é¡¹ç›®çš„ç»éªŒï¼š

| ä»»åŠ¡ | Python å®ç° | rustbpe å®ç° | æ€§èƒ½æå‡ |
|------|-------------|--------------|----------|
| BPE è®­ç»ƒ (1GB æ–‡æœ¬) | ~30 åˆ†é’Ÿ | ~5 åˆ†é’Ÿ | **6x** |
| å†…å­˜ä½¿ç”¨ | é«˜å³° ~8GB | é«˜å³° ~2GB | **4x** |
| CPU åˆ©ç”¨ç‡ | å•æ ¸ (å— GIL é™åˆ¶) | å¤šæ ¸å¹¶è¡Œ | **8x** (8æ ¸æœºå™¨) |

## ğŸ¯ ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ

### 1. **é€‚åˆç”¨ Rust å®ç°çš„åœºæ™¯**
- è®¡ç®—å¯†é›†å‹ç®—æ³• (å¦‚ BPE è®­ç»ƒ)
- å¤§è§„æ¨¡æ•°æ®å¤„ç†
- éœ€è¦å¹¶è¡Œè®¡ç®—çš„ä»»åŠ¡
- æ€§èƒ½å…³é”®çš„åº•å±‚ç®—æ³•

### 2. **ä¿ç•™ Python çš„åœºæ™¯**
- ä¸šåŠ¡é€»è¾‘å’Œèƒ¶æ°´ä»£ç 
- å¿«é€ŸåŸå‹å¼€å‘
- ä¸ Python ç”Ÿæ€ç³»ç»Ÿé›†æˆ
- ç”¨æˆ·æ¥å£å’Œé…ç½®

### 3. **æ··åˆæ¶æ„æ¨¡å¼**
```python
# æ¨èçš„ä½¿ç”¨æ¨¡å¼
class HybridTokenizer:
    def __init__(self):
        # Rust å®ç°çš„æ ¸å¿ƒç®—æ³•
        self.rust_tokenizer = rustbpe.Tokenizer()

        # Python å®ç°çš„è¾…åŠ©åŠŸèƒ½
        self.cache = {}
        self.config = {}

    def train(self, data):
        # è°ƒç”¨é«˜æ€§èƒ½ Rust å®ç°
        return self.rust_tokenizer.train_from_iterator(data)

    def preprocess(self, text):
        # Python å®ç°çš„è½»é‡çº§é¢„å¤„ç†
        return text.strip().lower()
```

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
PyO3 ä½¿å¾— Rust å’Œ Python çš„åä½œå˜å¾—æå…¶é«˜æ•ˆã€‚ä½ å¯ä»¥ç”¨ Rust é‡å†™æ€§èƒ½ç“¶é¢ˆï¼ŒåŒæ—¶ä¿æŒ Python çš„å¼€å‘æ•ˆç‡ã€‚è¿™ç§æ¨¡å¼åœ¨ AI/ML é¢†åŸŸè¶Šæ¥è¶Šæµè¡Œï¼Œå› ä¸ºè®­ç»ƒç®—æ³•é€šå¸¸è®¡ç®—å¯†é›†ï¼Œè€Œæ¨¡å‹æ¨ç†å’Œå®éªŒéœ€è¦ Python çš„çµæ´»æ€§ã€‚
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

æ€»ç»“ï¼šrustbpe é€šè¿‡ PyO3 å®ç°äº†å®Œç¾çš„ Python-Rust äº’æ“ä½œï¼Œè®©å¼€å‘è€…æ—¢èƒ½åœ¨ Python ä¸­äº«å— Rust çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œåˆèƒ½ä¿æŒ Python ç”Ÿæ€ç³»ç»Ÿçš„ä¾¿åˆ©æ€§ã€‚è¿™æ˜¯ç°ä»£ AI å·¥ç¨‹ä¸­ä¸€ä¸ªéå¸¸æˆåŠŸçš„è·¨è¯­è¨€é›†æˆæ¡ˆä¾‹ã€‚