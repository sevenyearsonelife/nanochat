# Chat Dialogue Log

## å¯¹è¯æ—¶é—´
2025-11-11 22:45:02

---

## å¯¹è¯å†…å®¹

### é—®é¢˜ 1
ç”¨æˆ·ï¼š@nanochat/tokenizer.py è¿™ä»½æ–‡ä»¶çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?å¦‚ä½•ä½¿ç”¨?è¯·ç»™å‡ºä½¿ç”¨çš„ç¤ºä¾‹,ç»™å‡ºè¾“å…¥å’Œè¾“å‡º.
è§£é‡Šå³å¯,ä¸ç”¨æ‰§è¡Œ.ä½ æ˜¯ä¸€ä½llmä¸“å®¶,è¯·æ·±åº¦æ€è€ƒ,ç»™å‡ºç”±æµ…å…¥æ·±çš„è§£é‡Š!

åŠ©æ‰‹ï¼šè¿™ä¸ª `tokenizer.py` æ–‡ä»¶æ˜¯ä¸€ä¸ªå®ç°äº† GPT-4 é£æ ¼ BPE (Byte-Pair Encoding) åˆ†è¯å™¨çš„æ ¸å¿ƒæ¨¡å—ã€‚è®©æˆ‘æ·±å…¥åˆ†æå®ƒçš„åŠŸèƒ½å’Œä½œç”¨ï¼š

## æ–‡ä»¶çš„æ ¸å¿ƒä½œç”¨

### 1. **æ–‡æœ¬æ•°å­—åŒ–è½¬æ¢**
åˆ†è¯å™¨å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸º token ID åºåˆ—ï¼Œè¿™æ˜¯è¯­è¨€æ¨¡å‹ç†è§£æ–‡æœ¬çš„åŸºç¡€ã€‚ä¾‹å¦‚ï¼š
- è¾“å…¥æ–‡æœ¬: "Hello, world!"
- è¾“å‡ºåºåˆ—: [15496, 11, 3186, 0] (ç¤ºä¾‹ ID)

### 2. **åŒé‡å®ç°æ¶æ„**
æ–‡ä»¶æä¾›äº†ä¸¤ç§å®ç°æ–¹æ¡ˆï¼š

**A. HuggingFace Tokenizer** (`HuggingFaceTokenizer` ç±»)
- é€‚åˆè®­ç»ƒå’Œæ¨ç†
- åŸºäº HuggingFace çš„ `tokenizers` åº“
- æ”¯æŒå®Œæ•´çš„ BPE è®­ç»ƒæµç¨‹

**B. RustBPE + Tiktoken ç»„åˆ** (`RustBPETokenizer` ç±»)
- é«˜æ•ˆæ¨ç†æ–¹æ¡ˆ
- Rust BPE è´Ÿè´£è®­ç»ƒï¼Œtiktoken è´Ÿè´£å¿«é€Ÿæ¨ç†
- æ€§èƒ½æ›´ä¼˜ï¼Œç‰¹åˆ«é€‚åˆç”Ÿäº§ç¯å¢ƒ

## æ·±åº¦åŠŸèƒ½åˆ†æ

### **ç‰¹æ®Šæ ‡è®°ç³»ç»Ÿ**
```python
SPECIAL_TOKENS = [
    "<|bos|>",              # æ–‡æ¡£å¼€å§‹æ ‡è®°
    "<|user_start|>", "<|user_end|>",          # ç”¨æˆ·æ¶ˆæ¯è¾¹ç•Œ
    "<|assistant_start|>", "<|assistant_end|>", # åŠ©æ‰‹å›å¤è¾¹ç•Œ
    "<|python_start|>", "<|python_end|>",      # Python ä»£ç æ‰§è¡Œ
    "<|output_start|>", "<|output_end|>",      # å·¥å…·è¾“å‡ºç»“æœ
]
```

### **æ™ºèƒ½åˆ†è¯æ¨¡å¼**
```python
SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,2}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
```

### **å¯¹è¯æ¸²æŸ“åŠŸèƒ½**
è¿™æ˜¯æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œ`render_conversation` æ–¹æ³•å°†ç»“æ„åŒ–å¯¹è¯è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®ã€‚

## ä½¿ç”¨ç¤ºä¾‹è¯¦è§£

### **ç¤ºä¾‹ 1: åŸºæœ¬åˆ†è¯**
```python
# è·å–åˆ†è¯å™¨
tokenizer = get_tokenizer()

# ç¼–ç æ–‡æœ¬
text = "Hello, world!"
token_ids = tokenizer.encode(text)
print(token_ids)  # è¾“å‡º: [9906, 11, 1917, 0]

# è§£ç æ–‡æœ¬
decoded = tokenizer.decode(token_ids)
print(decoded)  # è¾“å‡º: "Hello, world!"
```

### **ç¤ºä¾‹ 2: å¯¹è¯è®­ç»ƒæ•°æ®ç”Ÿæˆ**
```python
# å¤æ‚å¯¹è¯åŒ…å«å·¥å…·è°ƒç”¨
conversation = {
    "messages": [
        {"role": "user", "content": "è®¡ç®— 2+2"},
        {"role": "assistant", "content": [
            {"type": "text", "text": "æˆ‘æ¥å¸®ä½ è®¡ç®—ï¼š"},
            {"type": "python", "text": "print(2+2)"},
            {"type": "python_output", "text": "4"},
            {"type": "text", "text": "ç­”æ¡ˆæ˜¯ 4"}
        ]}
    ]
}

# æ¸²æŸ“ä¸ºè®­ç»ƒæ•°æ®
ids, mask = tokenizer.render_conversation(conversation)
```

### **ç¤ºä¾‹ 3: åˆ†è¯å™¨è®­ç»ƒ**
```python
# ä»æ–‡æœ¬æ•°æ®è®­ç»ƒæ–°åˆ†è¯å™¨
text_data = ["å¤§é‡æ–‡æœ¬æ•°æ®1", "å¤§é‡æ–‡æœ¬æ•°æ®2", ...]  # æ–‡æœ¬è¿­ä»£å™¨
new_tokenizer = RustBPETokenizer.train_from_iterator(text_data, vocab_size=32768)

# ä¿å­˜åˆ†è¯å™¨
new_tokenizer.save("path/to/tokenizer_dir")
```

### **ç¤ºä¾‹ 4: å¼ºåŒ–å­¦ä¹ çš„å®Œæˆæ¸²æŸ“**
```python
# ä¸ºå¼ºåŒ–å­¦ä¹ ç”Ÿæˆæç¤º
conversation = {
    "messages": [
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": ""}  # ç©ºå›å¤ï¼Œéœ€è¦æ¨¡å‹ç”Ÿæˆ
    ]
}

# æ¸²æŸ“ä¸ºç”Ÿæˆæç¤º
prompt_ids = tokenizer.render_for_completion(conversation)
```

## æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **æ··åˆæ¶æ„è®¾è®¡**ï¼šç»“åˆ RustBPE çš„è®­ç»ƒæ•ˆç‡å’Œ tiktoken çš„æ¨ç†é€Ÿåº¦
2. **å¯¹è¯ç»“æ„æ„ŸçŸ¥**ï¼šé€šè¿‡ç‰¹æ®Šæ ‡è®°å®ç°å¤šè½®å¯¹è¯å’Œå·¥å…·è°ƒç”¨çš„ç»“æ„åŒ–è¡¨ç¤º
3. **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨ `@lru_cache` ç¼“å­˜ç‰¹æ®Šæ ‡è®°ç¼–ç 
4. **æ‰¹å¤„ç†æ”¯æŒ**ï¼š`encode` æ–¹æ³•æ”¯æŒå•ä¸ªå­—ç¬¦ä¸²å’Œå­—ç¬¦ä¸²åˆ—è¡¨çš„æ‰¹å¤„ç†
5. **å¯è§†åŒ–è°ƒè¯•**ï¼š`visualize_tokenization` æ–¹æ³•ä¾¿äºè°ƒè¯•åˆ†è¯ç»“æœ

è¿™ä¸ªåˆ†è¯å™¨ä¸ä»…ä»…æ˜¯ç®€å•çš„æ–‡æœ¬æ•°å­—åŒ–å·¥å…·ï¼Œæ›´æ˜¯æ•´ä¸ªå¯¹è¯è®­ç»ƒå’Œæ¨ç†ä½“ç³»çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œä¸ºæ¨¡å‹æä¾›äº†ç»“æ„åŒ–çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚

---

### é—®é¢˜ 2
ç”¨æˆ·ï¼šSPECIAL_TOKENS = [
    # every document begins with the Beginning of Sequence (BOS) token that delimits documents
    "<|bos|>",
    # tokens below are only used during finetuning to render Conversations into token ids
    "<|user_start|>", # user messages
    "<|user_end|>",
    "<|assistant_start|>", # assistant messages
    "<|assistant_end|>",
    "<|python_start|>", # assistant invokes python REPL tool
    "<|python_end|>",
    "<|output_start|>", # python REPL outputs back to assistant
    "<|output_end|>",
]

---

ç»“åˆå…·ä½“çš„ç¤ºä¾‹è§£é‡Šè¿™äº›ç‰¹æ®Štokençš„ä½œç”¨!

åŠ©æ‰‹ï¼š## ç‰¹æ®Š Token çš„è®¾è®¡ç†å¿µä¸ä½œç”¨æœºåˆ¶

### **1. `<|bos|>` - æ–‡æ¡£è¾¹ç•Œæ ‡è®°**

**ä½œç”¨**: æ ‡è®°æ¯ä¸ªå¯¹è¯æ–‡æ¡£çš„å¼€å§‹ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡è¾¹ç•Œã€‚

**ç¤ºä¾‹**:
```python
# è¾“å…¥å¯¹è¯
conversation1 = {"messages": [{"role": "user", "content": "ä½ å¥½"}]}
conversation2 = {"messages": [{"role": "user", "content": "å¤©æ°”å¦‚ä½•ï¼Ÿ"}]}

# æ¸²æŸ“åçš„ token åºåˆ—
# conv1_ids: [<|bos|>, <|user_start|>, "ä½ å¥½", <|user_end|>]
# conv2_ids: [<|bos|>, <|user_start|>, "å¤©æ°”å¦‚ä½•ï¼Ÿ", <|user_end|>]
```

### **2. `<|user_start|>` å’Œ `<|user_end|>` - ç”¨æˆ·æ¶ˆæ¯è¾¹ç•Œ**

**ä½œç”¨**: æ˜ç¡®æ ‡è®°ç”¨æˆ·è¾“å…¥çš„å¼€å§‹å’Œç»“æŸï¼Œè®©æ¨¡å‹åŒºåˆ†ç”¨æˆ·å’ŒåŠ©æ‰‹çš„è§’è‰²ã€‚

**ç¤ºä¾‹**:
```python
# ç®€å•å¯¹è¯
conversation = {
    "messages": [
        {"role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ"},
        {"role": "assistant", "content": "æˆ‘æ˜¯AIåŠ©æ‰‹ã€‚"}
    ]
}

# æ¸²æŸ“ç»“æœ:
# [<|bos|>, <|user_start|>, "ä½ ", "æ˜¯", "è°", "ï¼Ÿ", <|user_end|>,
#  <|assistant_start|>, "æˆ‘", "æ˜¯", "AI", "åŠ©", "æ‰‹", "ã€‚", <|assistant_end|>]
```

### **3. `<|assistant_start|>` å’Œ `<|assistant_end|>` - åŠ©æ‰‹å›å¤è¾¹ç•Œ**

**ä½œç”¨**: æ ‡è®°åŠ©æ‰‹å›å¤çš„å¼€å§‹å’Œç»“æŸï¼Œé…åˆè®­ç»ƒæ©ç å®ç°ç›‘ç£å­¦ä¹ ã€‚

**è®­ç»ƒæ©ç ç¤ºä¾‹**:
```python
# render_conversation ç”Ÿæˆçš„è®­ç»ƒæ©ç 
tokens =  ["<|bos|>", "<|user_start|>", "ä½ å¥½", <|user_end|>,
          "<|assistant_start|>", "æˆ‘", "æ˜¯", "AI", <|assistant_end|>]
masks   =  [0,           0,              0,     0,
           0,             1,     1,   1,     0]

# mask=1 çš„ token éœ€è¦è®¡ç®—æŸå¤±å¹¶æ›´æ–°æ¢¯åº¦
# mask=0 çš„ token ä¸å‚ä¸æŸå¤±è®¡ç®—ï¼ˆç”¨æˆ·è¾“å…¥å’Œç‰¹æ®Šæ ‡è®°ï¼‰
```

### **4. `<|python_start|>` å’Œ `<|python_end|>` - Python ä»£ç æ‰§è¡Œæ ‡è®°**

**ä½œç”¨**: æ ‡è®°åŠ©æ‰‹è°ƒç”¨ Python å·¥å…·çš„ä»£ç æ®µï¼Œå®ç°å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚

**ç¤ºä¾‹**:
```python
conversation = {
    "messages": [
        {"role": "user", "content": "è®¡ç®— 3+5"},
        {"role": "assistant", "content": [
            {"type": "text", "text": "æˆ‘æ¥å¸®ä½ è®¡ç®—ï¼š"},
            {"type": "python", "text": "print(3+5)"},
            {"type": "text", "text": "ç»“æœæ˜¯ 8"}
        ]}
    ]
}

# æ¸²æŸ“ç»“æœ:
# [<|bos|>,
#  <|user_start|>, "è®¡ç®— 3+5", <|user_end|>,
#  <|assistant_start|>,
#     "æˆ‘æ¥å¸®ä½ è®¡ç®—ï¼š",
#     <|python_start|>, "print", "(", "3", "+", "5", ")", <|python_end|>,
#     "ç»“æœæ˜¯ 8",
#  <|assistant_end|>]
```

### **5. `<|output_start|>` å’Œ `<|output_end|>` - å·¥å…·è¾“å‡ºæ ‡è®°**

**ä½œç”¨**: æ ‡è®° Python ä»£ç æ‰§è¡Œçš„ç»“æœï¼Œè®©æ¨¡å‹ç†è§£å·¥å…·è¾“å‡ºçš„è¾¹ç•Œã€‚

**å®Œæ•´å·¥å…·è°ƒç”¨ç¤ºä¾‹**:
```python
conversation = {
    "messages": [
        {"role": "user", "content": "2çš„10æ¬¡æ–¹æ˜¯å¤šå°‘ï¼Ÿ"},
        {"role": "assistant", "content": [
            {"type": "text", "text": "æˆ‘æ¥è®¡ç®—ä¸€ä¸‹ï¼š"},
            {"type": "python", "text": "print(2**10)"},
            {"type": "python_output", "text": "1024"},
            {"type": "text", "text": "2çš„10æ¬¡æ–¹æ˜¯1024ã€‚"}
        ]}
    ]
}
```

**å®Œæ•´çš„ token åºåˆ—ç»“æ„**:
```
<|bos|>
<|user_start|>2çš„10æ¬¡æ–¹æ˜¯å¤šå°‘ï¼Ÿ<|user_end|>
<|assistant_start|>
    æˆ‘æ¥è®¡ç®—ä¸€ä¸‹ï¼š
    <|python_start|>print(2**10)<|python_end|>
    <|output_start|>1024<|output_end|>
    2çš„10æ¬¡æ–¹æ˜¯1024ã€‚
<|assistant_end|>
```

**è®­ç»ƒæ©ç åˆ†é…**:
```python
# å¯¹åº”çš„è®­ç»ƒæ©ç 
tokens = ["æˆ‘æ¥è®¡ç®—ä¸€ä¸‹ï¼š", <|python_start|>, "print", "...", <|python_end|>,
          <|output_start|>, "1024", <|output_end|>, "2çš„10æ¬¡æ–¹æ˜¯1024ã€‚"]
masks   = [1,                1,               1,         1,
          0,                 0,        0,         1]         # â† å·¥å…·è¾“å‡ºä¸è®­ç»ƒï¼

# ä¸ºä»€ä¹ˆå·¥å…·è¾“å‡ºçš„ mask=0ï¼Ÿ
# å› ä¸ºè¿™äº›è¾“å‡ºæ˜¯ Python æ‰§è¡Œçš„ç»“æœï¼Œä¸æ˜¯æ¨¡å‹ç”Ÿæˆçš„
# æ¨¡å‹éœ€è¦å­¦ä¼šè°ƒç”¨å·¥å…·ï¼Œä½†è¦å¿½ç•¥å·¥å…·çš„å…·ä½“è¾“å‡ºå†…å®¹
```

## å¤šè½®å¯¹è¯çš„å¤æ‚ç¤ºä¾‹

```python
conversation = {
    "messages": [
        {"role": "user", "content": "å¸®æˆ‘è§£æ–¹ç¨‹ x + 5 = 12"},
        {"role": "assistant", "content": [
            {"type": "text", "text": "æˆ‘æ¥è§£è¿™ä¸ªæ–¹ç¨‹ï¼š"},
            {"type": "python", "text": "x = 12 - 5\nprint(x)"},
            {"type": "python_output", "text": "7"},
            {"type": "text", "text": "æ‰€ä»¥ x = 7ã€‚"}
        ]},
        {"role": "user", "content": "éªŒè¯ä¸€ä¸‹ï¼š7 + 5 ç­‰äºå¤šå°‘ï¼Ÿ"},
        {"role": "assistant", "content": [
            {"type": "text", "text": "è®©æˆ‘éªŒè¯ä¸€ä¸‹ï¼š"},
            {"type": "python", "text": "print(7 + 5)"},
            {"type": "python_output", "text": "12"},
            {"type": "text", "text": "éªŒè¯æ­£ç¡®ï¼7 + 5 = 12ã€‚"}
        ]}
    ]
}
```

é€šè¿‡è¿™äº›è¯¦ç»†çš„ç¤ºä¾‹ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™äº›ç‰¹æ®Š token ä¸ä»…ä»…æ˜¯ç®€å•çš„åˆ†éš”ç¬¦ï¼Œè€Œæ˜¯æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„å¯¹è¯å’Œå·¥å…·ä½¿ç”¨æ¡†æ¶ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿï¼š

- **ç†è§£å¯¹è¯ç»“æ„**ï¼šçŸ¥é“è°åœ¨è¯´è¯ï¼Œä»€ä¹ˆæ—¶å€™è½®åˆ°è‡ªå·±
- **å­¦ä¼šå·¥å…·è°ƒç”¨**ï¼šåœ¨åˆé€‚çš„æ—¶æœºä½¿ç”¨ Python å·¥å…·è§£å†³é—®é¢˜
- **å¤„ç†å·¥å…·ç»“æœ**ï¼šç†è§£å·¥å…·è¾“å‡ºçš„å«ä¹‰å¹¶ç»§ç»­å¯¹è¯
- **ä¿æŒä¸Šä¸‹æ–‡è¿è´¯**ï¼šåœ¨å¤šè½®å¯¹è¯ä¸­ç»´æŒä¸€è‡´çš„è§’è‰²è®¤çŸ¥

è¿™ç§è®¾è®¡æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹å®ç°å¯¹è¯å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å…³é”®æŠ€æœ¯åŸºç¡€ã€‚

---

### é—®é¢˜ 3
ç”¨æˆ·ï¼šSPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,2}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

---

ç»“åˆå…·ä½“çš„ç¤ºä¾‹è§£é‡ŠSPLIT_PATTERNçš„ä½œç”¨!

åŠ©æ‰‹ï¼šè®©æˆ‘æ·±å…¥è§£æè¿™ä¸ªå¤æ‚çš„æ­£åˆ™è¡¨è¾¾å¼ `SPLIT_PATTERN`ï¼Œè¿™æ˜¯å®ç° GPT-4 é£æ ¼æ™ºèƒ½åˆ†è¯çš„æ ¸å¿ƒç»„ä»¶ï¼š

## SPLIT_PATTERN çš„ç»“æ„åˆ†è§£

è®©æˆ‘ä»¬æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºåˆ†è§£è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼š

```python
SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,2}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
```

### **1. è‹±æ–‡ç¼©å†™å¤„ç†** - `'(?i:[sdmt]|ll|ve|re)`

**ä½œç”¨**: æ™ºèƒ½è¯†åˆ«å¸¸è§è‹±æ–‡ç¼©å†™ï¼Œå°†å®ƒä»¬ä½œä¸ºç‹¬ç«‹çš„ token

```python
# ç¤ºä¾‹æ–‡æœ¬: "I don't like it, but she'll and we've been there"
# åŒ¹é…ç»“æœ:
# - "don't" åˆ†ä¸º ["don", "'t"] (åŒ¹é… 't)
# - "she'll" åˆ†ä¸º ["she", "'ll"] (åŒ¹é… 'll)
# - "we've" åˆ†ä¸º ["we", "'ve"] (åŒ¹é… 've)
# - "I" åˆ†ä¸º ["I"] (å•ç‹¬å¤„ç†)
# - "like" åˆ†ä¸º ["like"]
# - "it" åˆ†ä¸º ["it"]
# - "but" åˆ†ä¸º ["but"]
# - "been" åˆ†ä¸º ["been"]
# - "there" åˆ†ä¸º ["there"]

# å®Œæ•´ tokenization:
# ["I", " don", "'t", " like", " it", ",", " but", " she", "'ll", " and", " we", "'ve", " been", " there"]
```

### **2. å•è¯è¯†åˆ«** - `[^\r\n\p{L}\p{N}]?+\p{L}+`

**ä½œç”¨**: è¯†åˆ«å®Œæ•´çš„å­—æ¯å•è¯ï¼Œå¯èƒ½å‰é¢æœ‰éå­—æ¯æ•°å­—å­—ç¬¦

```python
# ç¤ºä¾‹: "Hello! (world) $100"
# åŒ¹é…é€»è¾‘:
# - "Hello" åŒ¹é…: [^\r\n\p{L}\p{N}]?+\p{L}+ (æ²¡æœ‰å‰å¯¼éå­—æ¯å­—ç¬¦)
# - "!" è¢«å…¶ä»–è§„åˆ™åŒ¹é…
# - "(world)" ä¸­çš„ "world" åŒ¹é…: [^\r\n\p{L}\p{N}]?+\p{L}+ (å‰å¯¼å­—ç¬¦ '(')
# - "$" è¢«å…¶ä»–è§„åˆ™åŒ¹é…
# - "100" è¢«æ•°å­—è§„åˆ™åŒ¹é…

# Tokenization ç»“æœ:
# ["Hello", "!", "(", "world", ")", "$", "100"]
```

### **3. æ•°å­—å¤„ç†** - `\p{N}{1,2}`

**ä½œç”¨**: å°†æ•°å­—é™åˆ¶ä¸º 1-2 ä½ï¼Œä¼˜åŒ– token ä½¿ç”¨æ•ˆç‡

```python
# ç¤ºä¾‹æ–‡æœ¬: "The year is 2024, price is $19.99"
# ä¼ ç»Ÿåˆ†è¯å¯èƒ½ä¼šå°† "2024" åˆ†ä¸º ["2024"]
# è¿™ä¸ªæ¨¡å¼ä¼šåˆ†ä¸º ["20", "24"] (å› ä¸ºæœ€å¤š2ä½)

# Tokenization ç»“æœ:
# ["The", " year", " is", " 20", "24", ",", " price", " is", " $", "19", ".", "99"]

# ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
# - å‡å°‘é•¿æ•°å­—çš„ token æµªè´¹
# - å¯¹äºå°æ¨¡å‹æ›´å‹å¥½
# - ä¸Šä¸‹æ–‡é€šå¸¸èƒ½é‡æ„åŸå§‹æ•°å­—
```

### **4. æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦** - ` ?[^\s\p{L}\p{N}]++[\r\n]*`

**ä½œç”¨**: å¤„ç†æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œå¯èƒ½å‰å¯¼ç©ºæ ¼ï¼Œå¯èƒ½åè·Ÿæ¢è¡Œ

```python
# ç¤ºä¾‹: "Hello, world!\nHow are you?"
# åŒ¹é…é€»è¾‘:
# - "," åŒ¹é…: [^\s\p{L}\p{N}]++ (é€—å·)
# - "!" åŒ¹é…: [^\s\p{L}\p{N}]++[\r\n]* (æ„Ÿå¹å·åè·Ÿæ¢è¡Œ)
# - "?" åŒ¹é…: [^\s\p{L}\p{N}]++ (é—®å·)

# Tokenization:
# ["Hello", ",", " world", "!", "\n", "How", " are", " you", "?"]
```

### **5. æ¢è¡Œå¤„ç†** - `\s*[\r\n]`

**ä½œç”¨**: ä¸“é—¨å¤„ç†æ¢è¡Œç¬¦å’Œå‘¨å›´çš„ç©ºç™½

```python
# ç¤ºä¾‹: "Line 1\n\nLine 2\t\r\nLine 3"
# åŒ¹é…ç»“æœ:
# - ç¬¬ä¸€ä¸ª "\n" åŒ¹é… \s*[\r\n]
# - ç¬¬äºŒä¸ª "\n\n" ä¸­çš„ç¬¬ä¸€ä¸ª "\n" åŒ¹é…ï¼Œç¬¬äºŒä¸ªè¢«å…¶ä»–è§„åˆ™å¤„ç†
# - "\t\r\n" æ•´ä½“åŒ¹é… \s*[\r\n]

# Tokenization:
# ["Line", " 1", "\n", "\n", "Line", " 2", "\t\r\n", "Line", " 3"]
```

### **6. å¤æ‚ç©ºç™½å¤„ç†** - `\s+(?!\S)|\s+`

**ä½œç”¨**: å¤„ç†å„ç§ç©ºç™½å­—ç¬¦åºåˆ—

```python
# ç¤ºä¾‹: "Hello   \t  world"
# \s+(?!\S): åŒ¹é…ä¸ä»¥éç©ºç™½å­—ç¬¦ç»“å°¾çš„ç©ºç™½åºåˆ—
# \s+: åŒ¹é…å‰©ä½™çš„ç©ºç™½å­—ç¬¦

# Tokenization:
# ["Hello", "   \t  ", "world"]
```

## å®Œæ•´ç¤ºä¾‹åˆ†æ

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¤æ‚ä¾‹å­çœ‹å®Œæ•´çš„åˆ†è¯è¿‡ç¨‹ï¼š

```python
text = "I can't believe it's 2024! The price is $19.99.\nNew product? Yes!"

# åˆ†æ­¥è§£æ:
# 1. '(?i:[sdmt]|ll|ve|re) åŒ¹é…:
#    - "can't" -> ["can", "'t"]
#    - "it's" -> ["it", "'s"]

# 2. [^\r\n\p{L}\p{N}]?+\p{L}+ åŒ¹é…:
#    - "I" -> ["I"]
#    - "believe" -> ["believe"]
#    - "The" -> ["The"]
#    - "price" -> ["price"]
#    - "is" -> ["is"]
#    - "New" -> ["New"]
#    - "product" -> ["product"]
#    - "Yes" -> ["Yes"]

# 3. \p{N}{1,2} åŒ¹é…:
#    - "2024" -> ["20", "24"]
#    - "19" -> ["19"]
#    - "99" -> ["99"]

# 4. æ ‡ç‚¹å’Œå…¶ä»–è§„åˆ™:
#    - "!" -> ["!"]
#    - "$" -> ["$"]
#    - "." -> ["."]
#    - "\n" -> ["\n"]
#    - "?" -> ["?"]

# æœ€ç»ˆ tokenization:
tokens = [
    "I", " can", "'t", " believe", " it", "'s", " 20", "24",
    "!", " The", " price", " is", " $", "19", ".", "99", ".",
    "\n", "New", " product", "?", " Yes", "!"
]
```

## æ€§èƒ½å¯¹æ¯”

```python
# ä¼ ç»Ÿç®€å•åˆ†è¯ vs SPLIT_PATTERN
text = "Hello, I'm interested in the $19.99 product from 2023."

# ä¼ ç»Ÿå­—ç¬¦çº§åˆ†è¯ (å¤ªå¤š token):
# ["H", "e", "l", "l", "o", ",", " ", "I", "'", "m", " ", ...]  # ~35 tokens

# ç®€å•è¯çº§åˆ†è¯ (è¯­ä¹‰æŸå¤±):
# ["Hello", ",", "I", "'", "m", "interested", "in", "the", "$", "19.99", "product", "from", "2023", "."]  # æ•°å­—å’Œç¼©å†™å¤„ç†ä¸å½“

# SPLIT_PATTERN æ™ºèƒ½åˆ†è¯ (æœ€ä¼˜å¹³è¡¡):
# ["Hello", ",", " I", "'m", " interested", " in", " the", " $", "19", ".", "99", " product", " from", " 20", "23", "."]  # ~16 tokens
```

## è®¾è®¡ä¼˜åŠ¿æ€»ç»“

1. **è¯­è¨€å­¦æ„ŸçŸ¥**: æ­£ç¡®å¤„ç†è‹±æ–‡ç¼©å†™ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
2. **æ•°å­—ä¼˜åŒ–**: é™åˆ¶æ•°å­—é•¿åº¦ï¼Œæé«˜å°æ¨¡å‹æ•ˆç‡
3. **æ ‡ç‚¹æ™ºèƒ½**: åˆç†å¤„ç†æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
4. **å¤šè¯­è¨€æ”¯æŒ**: é€šè¿‡ Unicode ç±»æ”¯æŒå„ç§è¯­è¨€å­—ç¬¦
5. **æ€§èƒ½å¹³è¡¡**: åœ¨è¯­ä¹‰å®Œæ•´æ€§å’Œ token æ•ˆç‡é—´æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡

è¿™ä¸ªåˆ†è¯æ¨¡å¼æ˜¯ç°ä»£è¯­è¨€æ¨¡å‹åˆ†è¯æŠ€æœ¯çš„ç²¾ç²¹ä½“ç°ï¼Œå®ƒä¸ä»…ä»…æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†ï¼Œæ›´æ˜¯åŸºäºè¯­è¨€å­¦å’Œå·¥ç¨‹ä¼˜åŒ–çš„æ™ºèƒ½æ–‡æœ¬å¤„ç†ç³»ç»Ÿã€‚

---

### é—®é¢˜ 4
ç”¨æˆ·ï¼šè¿™é‡Œçš„SPLIT_PATTERNå°±æ˜¯å°†ç»™å®šå­—ç¬¦ä¸²tokenåŒ–çš„è§„åˆ™ä¹ˆ?

åŠ©æ‰‹ï¼šä¸æ˜¯çš„ï¼è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„è¯¯è§£ã€‚è®©æˆ‘æ¾„æ¸…ä¸€ä¸‹ï¼š

## ä¸¤é˜¶æ®µåˆ†è¯æµç¨‹

### **é˜¶æ®µ 1: SPLIT_PATTERN - é¢„åˆ†å‰²**
```python
SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,2}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
```

**ä½œç”¨**: å°†æ–‡æœ¬æŒ‰è¯­è¨€å­¦è§„åˆ™åˆ†å‰²æˆå€™é€‰ç‰‡æ®µ
```python
text = "Hello, world!"
split_result = ["Hello", ",", " world", "!"]  # â† è¿™æ˜¯ SPLIT_PATTERN çš„ç»“æœ
```

### **é˜¶æ®µ 2: BPE ç®—æ³• - çœŸæ­£çš„ token åŒ–**
```python
# BPE è¯æ±‡è¡¨ç¤ºæ„
vocab = {
    "Hello": 15496,
    ",",    11,
    " world": 264,
    "!": 0,
    "He": 1423,
    "llo": 6231,
    # ... æ›´ç»†ç²’åº¦çš„ subword
}
```

## å®Œæ•´çš„åˆ†è¯è¿‡ç¨‹

```python
# è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ›´å¤æ‚çš„ä¾‹å­
text = "Hello, unlikely!"

# é˜¶æ®µ 1: SPLIT_PATTERN é¢„åˆ†å‰²
split_chunks = ["Hello", ",", " unlikely", "!"]

# é˜¶æ®µ 2: BPE è¿›ä¸€æ­¥åˆ†è¯
# å¦‚æœ "unlikely" ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼ŒBPE ä¼šè¿›ä¸€æ­¥åˆ†å‰²ï¼š
final_tokens = ["Hello", ",", " un", "like", "ly", "!"]
# å¯¹åº”çš„ token IDs: [15496, 11, 778, 326, 67, 0]
```

## å®é™…ä»£ç ä¸­çš„ä½“ç°

åœ¨ `tokenizer.py` ä¸­å¯ä»¥çœ‹åˆ°è¿™ä¸ªä¸¤é˜¶æ®µè®¾è®¡ï¼š

```python
# HuggingFaceTokenizer ä¸­çš„é…ç½®
tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
    pre_tokenizers.Split(pattern=Regex(SPLIT_PATTERN), behavior="isolated", invert=False),
    pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=False)
])
```

**è§£æ**:
1. `Split(pattern=SPLIT_PATTERN)`: ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å¼é¢„åˆ†å‰²
2. `ByteLevel()`: å°†åˆ†å‰²ç»“æœè½¬æ¢ä¸ºå­—èŠ‚çº§åˆ«ï¼Œä¸º BPE åšå‡†å¤‡

## å…·ä½“ç¤ºä¾‹å¯¹æ¯”

è®©æˆ‘ç”¨ä¸€ä¸ªå®é™…ä¾‹å­è¯´æ˜åŒºåˆ«ï¼š

```python
# ç¤ºä¾‹æ–‡æœ¬
text = "programming"

# å¦‚æœåªæœ‰ SPLIT_PATTERN:
split_result = ["programming"]  # ä½œä¸ºä¸€ä¸ªæ•´ä½“

# ç»è¿‡ BPE ç®—æ³•åçš„çœŸæ­£åˆ†è¯:
bpe_tokens = ["program", "ming"]  # è¢«åˆ†å‰²ä¸ºä¸¤ä¸ª subword tokens

# æœ€ç»ˆ token IDs:
# "program" -> 2847
# "ming" -> 362
final_ids = [2847, 362]
```

## æ›´å¤æ‚çš„å¤šè¯­è¨€ç¤ºä¾‹

```python
text = "internationalization ğŸš€"

# SPLIT_PATTERN åˆ†å‰²ç»“æœ:
split_chunks = ["internationalization", " ", "ğŸš€"]

# BPE è¿›ä¸€æ­¥å¤„ç†:
final_tokens = ["international", "ization", " ", "ğŸš€"]  # é•¿å•è¯è¢«æ™ºèƒ½åˆ†å‰²

# Token IDs:
final_ids = [2456, 1278, 220, 38451]  # æ³¨æ„: ğŸš€ å¯èƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„ token
```

## ä¸ºä»€ä¹ˆéœ€è¦ä¸¤é˜¶æ®µï¼Ÿ

1. **æ•ˆç‡**: BPE ç®—æ³•åœ¨é¢„åˆ†å‰²åçš„ç‰‡æ®µä¸Šå·¥ä½œæ›´å¿«
2. **è¯­è¨€å­¦æ„ŸçŸ¥**: SPLIT_PATTERN ç¡®ä¿é‡è¦çš„è¯­è¨€å­¦è¾¹ç•Œï¼ˆå¦‚ç¼©å†™ï¼‰ä¸è¢«ç ´å
3. **è¯è¡¨ä¼˜åŒ–**: BPE å¯ä»¥ä¸“æ³¨äº subword çº§åˆ«çš„åˆå¹¶

```python
# é”™è¯¯çš„å•é˜¶æ®µç¤ºä¾‹:
# å¦‚æœæ²¡æœ‰ SPLIT_PATTERNï¼ŒBPE å¯èƒ½é”™è¯¯åœ°å¤„ç†:
bad_case = "don't"  # å¯èƒ½è¢«åˆ†å‰²ä¸º ["don", "'", "t"] - å¤±å»ç¼©å†™è¯­ä¹‰

# æ­£ç¡®çš„ä¸¤é˜¶æ®µå¤„ç†:
good_case = "don't"
# SPLIT_PATTERN: ["don", "'t"]  # ä¿æŒç¼©å†™å®Œæ•´æ€§
# BPE: ["don", "'t"]  # ä¸ä¼šè¿›ä¸€æ­¥åˆ†å‰²
```

## æ€»ç»“

**SPLIT_PATTERN çš„çœŸæ­£ä½œç”¨**ï¼š
- âœ… æ˜¯**é¢„åˆ†å‰²è§„åˆ™**ï¼Œä¸æ˜¯æœ€ç»ˆçš„ token åŒ–
- âœ… ç¡®ä¿**è¯­è¨€å­¦è¾¹ç•Œ**çš„æ­£ç¡®å¤„ç†
- âœ… ä¸º BPE ç®—æ³•æä¾›**é«˜è´¨é‡çš„è¾“å…¥ç‰‡æ®µ**
- âœ… æé«˜åˆ†è¯çš„**è¯­ä¹‰å®Œæ•´æ€§**

**æœ€ç»ˆçš„ token åŒ–**æ˜¯ç”± BPE (Byte-Pair Encoding) ç®—æ³•å®Œæˆçš„ï¼Œå®ƒä¼šåœ¨ SPLIT_PATTERN é¢„åˆ†å‰²çš„åŸºç¡€ä¸Šï¼Œæ ¹æ®è®­ç»ƒå¥½çš„è¯æ±‡è¡¨è¿›è¡Œè¿›ä¸€æ­¥çš„ subword åˆ†å‰²ã€‚

è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡ç»“åˆäº†è¯­è¨€å­¦çš„ç²¾ç¡®æ€§å’Œç»Ÿè®¡æ–¹æ³•çš„çµæ´»æ€§ï¼Œæ˜¯ç°ä»£åˆ†è¯å™¨å®ç°çš„æœ€ä½³å®è·µã€‚

---

### é—®é¢˜ 5
ç”¨æˆ·ï¼š SPLIT_PATTERN ç¡®å®å®šä¹‰äº† tokenizer åœ¨æ­£å¼è¿›å…¥ BPE åˆå¹¶ä¹‹å‰"ç¬¬ä¸€æ­¥æ€ä¹ˆåˆ‡ç‰‡"çš„è§„åˆ™â€”â€”å®ƒå‘Šè¯‰é¢„åˆ†è¯å™¨å¦‚ä½•æŠŠåŸå§‹å­—ç¬¦ä¸²æ‹†æˆæ›´å°çš„å­—æ¯æ®µã€ç¼©å†™ã€æ•°å­—ã€ç¬¦å·ã€ç©ºç™½ç­‰ç‰‡æ®µã€‚ä¸è¿‡ nanoChat çš„ tokenization è¿˜åŒ…å«ç¬¬äºŒæ­¥ï¼šåœ¨è¿™äº›ç‰‡æ®µå†…éƒ¨ç»§ç»­è¿è¡Œ BPEï¼ˆæˆ– rustbpe+tiktokenï¼‰çš„åˆå¹¶é€»è¾‘ï¼Œæ‰èƒ½å¾—åˆ°æœ€ç»ˆçš„ token idã€‚å› æ­¤å¯ä»¥ç†è§£ä¸ºï¼šSPLIT_PATTERN å†³å®šé¢„åˆ†è¯ç²’åº¦å’Œè¯­ä¹‰è¾¹ç•Œï¼Œæ˜¯æ•´ä½“ tokenization æµç¨‹ä¸­å¿…ä¸å¯å°‘çš„"ç²—åˆ‡"è§„åˆ™ï¼Œä½†æœ€ç»ˆ tokençš„å½¢æˆè¿˜è¦ç»“åˆåç»­çš„ BPE è®­ç»ƒä¸ç‰¹æ®Š tokenæ˜ å°„ã€‚

  ------

  æœ‰ä¸¤ä¸ªé—®é¢˜ä¸æ˜¯å¾ˆç†è§£ï¼š
  1.è¯·ç»“åˆå…·ä½“çš„ç¤ºä¾‹è§£é‡Štokenizerçš„ä¸¤ä¸ªæ­¥éª¤ï¼Œéœ€è¦é˜è¿°æ¯ä¸€æ­¥çš„è¾“å…¥å’Œè¾“å‡ºã€‚
  2.tokenizationç›´æ¥æ ¹æ®è§„åˆ™åˆ†å‰²å¥½ä¸å°±å¯ä»¥äº†ä¹ˆï¼Ÿä¸ºä½•è¿˜éœ€è¦è®­ç»ƒå‘¢ï¼Ÿæˆ‘æ„Ÿåˆ°å¾ˆéš¾ç†è§£ã€‚

åŠ©æ‰‹ï¼šè¿™ä¸¤ä¸ªé—®é¢˜éå¸¸æ ¸å¿ƒï¼è®©æˆ‘ç”¨å…·ä½“çš„ä¾‹å­æ¥æ·±å…¥è§£é‡Šï¼š

## é—®é¢˜1ï¼šTokenizer çš„ä¸¤æ­¥æµç¨‹è¯¦è§£

### **æ­¥éª¤1ï¼šé¢„åˆ†å‰² (Pre-tokenization)**
**è¾“å…¥**: åŸå§‹å­—ç¬¦ä¸²
**è¾“å‡º**: è¯­è¨€å­¦ç‰‡æ®µåˆ—è¡¨

```python
# å…·ä½“ç¤ºä¾‹
text = "I can't believe programming costs $1999.99!"

# SPLIT_PATTERN é¢„åˆ†å‰²
split_chunks = [
    "I",           # [^\r\n\p{L}\p{N}]?+\p{L}+ åŒ¹é…
    " ",           # \s+ åŒ¹é…ç©ºç™½
    "can",         # å•è¯éƒ¨åˆ†
    "'",           # ç¼©å†™å¤„ç†
    "t",           # ç¼©å†™å¤„ç†
    " ",           # ç©ºç™½
    "believe",     # å•è¯
    " ",           # ç©ºç™½
    "programming", # å•è¯
    " ",           # ç©ºç™½
    "costs",       # å•è¯
    " ",           # ç©ºç™½
    "$",           # ç¬¦å·
    "19",          # \p{N}{1,2} åŒ¹é…å‰ä¸¤ä½æ•°å­—
    "99",          # \p{N}{1,2} åŒ¹é…åä¸¤ä½æ•°å­—
    ".",           # æ ‡ç‚¹
    "!",           # æ ‡ç‚¹
]

# æ­¥éª¤1è¾“å‡º: ["I", " ", "can", "'", "t", " ", "believe", " ", "programming", " ", "costs", " ", "$", "19", "99", ".", "!"]
```

### **æ­¥éª¤2ï¼šBPE åˆå¹¶ (Byte-Pair Encoding)**
**è¾“å…¥**: é¢„åˆ†å‰²çš„ç‰‡æ®µåˆ—è¡¨
**è¾“å‡º**: æœ€ç»ˆçš„ token ID åºåˆ—

```python
# å‡è®¾æˆ‘ä»¬çš„BPEè¯æ±‡è¡¨åŒ…å«ä»¥ä¸‹mergeè§„åˆ™ï¼š
vocab = {
    # å•å­—ç¬¦
    "I": 15496, " ": 220, "c": 298, "a": 265, "n": 292,
    "'": 503, "t": 316, "b": 614, "e": 294, "l": 297,
    "v": 474, "p": 386, "r": 287, "o": 322, "g": 311,
    "m": 330, "i": 289, "s": 338, "$": 384, "9": 526,
    ".": 13, "!": 0,

    # åˆå¹¶åçš„é«˜é¢‘subword (è®­ç»ƒå¾—åˆ°)
    "can": 2847,      # "c" + "an" ç»å¸¸ä¸€èµ·å‡ºç°
    "bel": 5621,      # "b" + "el" ç»å¸¸ä¸€èµ·å‡ºç°
    "ieve": 8934,     # "ie" + "ve" ç»å¸¸ä¸€èµ·å‡ºç°
    "programm": 2341, # é«˜é¢‘subword
    "ing": 452,       # é«˜é¢‘åç¼€
    "cost": 1247,     # é«˜é¢‘è¯
    "s": 338,         # å•ç‹¬çš„s
    "19": 6234,       # å¸¸è§æ•°å­—
    "99": 8912,       # å¸¸è§æ•°å­—
}

# BPEç®—æ³•é€ä¸€å¤„ç†æ¯ä¸ªç‰‡æ®µï¼š
final_tokens = []

# å¤„ç† "I" â†’ [I] â†’ [15496]
final_tokens.extend([15496])

# å¤„ç† " " â†’ [ ] â†’ [220]
final_tokens.extend([220])

# å¤„ç† "can" â†’ ["c", "a", "n"] â†’ ["ca", "n"] â†’ ["can"] â†’ [2847]
final_tokens.extend([2847])

# å¤„ç† "'" â†’ ["'"] â†’ [503]
final_tokens.extend([503])

# å¤„ç† "t" â†’ ["t"] â†’ [316]
final_tokens.extend([316])

# å¤„ç† " " â†’ [220]
final_tokens.extend([220])

# å¤„ç† "believe" â†’ ["b", "e", "l", "i", "e", "v", "e"]
# â†’ ["be", "l", "i", "e", "v", "e"]
# â†’ ["bel", "i", "e", "v", "e"]
# â†’ ["bel", "ie", "v", "e"]
# â†’ ["bel", "ie", "ve"]
# â†’ ["bel", "ieve"]
final_tokens.extend([5621, 8934])

# å¤„ç† "programming" â†’ ["programm", "ing"]
final_tokens.extend([2341, 452])

# å¤„ç† " " â†’ [220]
final_tokens.extend([220])

# å¤„ç† "costs" â†’ ["cost", "s"]
final_tokens.extend([1247, 338])

# å¤„ç† " " â†’ [220]
final_tokens.extend([220])

# å¤„ç† "$" â†’ [384]
final_tokens.extend([384])

# å¤„ç† "19" â†’ [6234]
final_tokens.extend([6234])

# å¤„ç† "99" â†’ [8912]
final_tokens.extend([8912])

# å¤„ç† "." â†’ [13]
final_tokens.extend([13])

# å¤„ç† "!" â†’ [0]
final_tokens.extend([0])

# æœ€ç»ˆè¾“å‡º: [15496, 220, 2847, 503, 316, 220, 5621, 8934, 220, 2341, 452, 220, 1247, 338, 220, 384, 6234, 8912, 13, 0]
```

## é—®é¢˜2ï¼šä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒè€Œä¸çº¯é è§„åˆ™ï¼Ÿ

### **çº¯è§„åˆ™æ–¹æ³•çš„å±€é™**

å‡è®¾æˆ‘ä»¬åªç”¨è§„åˆ™åˆ†å‰²ï¼ˆå¦‚æ¯ä¸ªå•è¯ç‹¬ç«‹ï¼‰ï¼š

```python
# çº¯è§„åˆ™æ–¹æ³•ï¼šæ¯ä¸ªå•è¯/ç¬¦å·ä½œä¸ºä¸€ä¸ªtoken
text1 = "I like programming"
text2 = "I love programming"
text3 = "I enjoy programming"

# çº¯è§„åˆ™tokenization:
rules_tokens1 = ["I", "like", "programming"]        # 3 tokens
rules_tokens2 = ["I", "love", "programming"]        # 3 tokens
rules_tokens3 = ["I", "enjoy", "programming"]       # 3 tokens

# é—®é¢˜ï¼šæ¯ä¸ªå•è¯éƒ½ç‹¬ç«‹ï¼Œæ— æ³•å…±äº«ä»»ä½•ä¿¡æ¯ï¼
```

### **BPEè®­ç»ƒçš„ä¼˜åŠ¿ï¼šæ•°æ®é©±åŠ¨åˆå¹¶**

```python
# è®­ç»ƒæ•°æ®åŒ…å«å¤§é‡æ–‡æœ¬åï¼ŒBPEå‘ç°ç»Ÿè®¡è§„å¾‹ï¼š
training_data = [
    "I like programming", "I love programming",
    "They enjoy programming", "We enjoy programming",
    "programmer", "programmable", "microprogramming"
    # ... æ•°ç™¾ä¸‡æ¡æ•°æ®
]

# BPEç®—æ³•ç»Ÿè®¡åå‘ç°çš„é«˜é¢‘æ¨¡å¼ï¼š
high_frequency_patterns = {
    "pro": "program", "programm", "programmer" ä¸­éƒ½å‡ºç°
    "gram": "program", "grammar", "diagram" ä¸­éƒ½å‡ºç°
    "ing": "programming", "enjoying", "loving" ä¸­éƒ½å‡ºç°
    "en": "enjoy", "enable", "enter" ä¸­éƒ½å‡ºç°
}

# BPEè®­ç»ƒç»“æœï¼ˆéƒ¨åˆ†è¯æ±‡è¡¨ï¼‰ï¼š
trained_vocab = {
    # åŸºç¡€å­—ç¬¦
    "I": 15496, " ": 220, "l": 297, "i": 289, "k": 331, "e": 294,
    "p": 386, "r": 287, "o": 322, "g": 311, "a": 265, "m": 330,
    "n": 292, "j": 364, "y": 320,

    # è®­ç»ƒå¾—åˆ°çš„é«˜é¢‘subwordï¼ˆè§„åˆ™æ–¹æ³•æ— æ³•å‘ç°ï¼ï¼‰
    "like": 2345,      # é«˜é¢‘åŠ¨è¯
    "love": 1567,      # é«˜é¢‘åŠ¨è¯
    "enjoy": 892,      # é«˜é¢‘åŠ¨è¯
    "programm": 567,   # è¯å¹²ï¼Œå¤šä¸ªè¯å…±äº«
    "ing": 452,        # é«˜é¢‘åç¼€ï¼Œæ‰€æœ‰åŠ¨è¯å…±äº«
    "en": 789,         # é«˜é¢‘å‰ç¼€
    "joy": 1234,       # è¯æ ¹ï¼Œ"enjoy"çš„ä¸€éƒ¨åˆ†
}
```

### **å¯¹æ¯”æ•ˆæœï¼šè§„åˆ™ vs è®­ç»ƒ**

```python
# å¤„ç†ä¸€ä¸ªæ–°è¯ï¼šprogrammable
text = "This device is programmable"

# çº¯è§„åˆ™æ–¹æ³•ï¼š
rules_result = ["This", "device", "is", "programmable"]  # 4 tokens
# é—®é¢˜ï¼š"programmable" æ˜¯æ–°è¯ï¼Œå¯èƒ½ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼é‡åˆ° OOV (Out-Of-Vocabulary)

# BPEè®­ç»ƒæ–¹æ³•ï¼š
bpe_result = ["This", "device", "is", "programm", "able"]  # 5 tokens
# ä¼˜åŠ¿ï¼šå³ä½¿"programmable"æ˜¯è®­ç»ƒæ—¶æ²¡è§è¿‡çš„è¯ï¼Œ
#      ä¹Ÿèƒ½é€šè¿‡å·²çŸ¥çš„ "programm" + "able" æ­£ç¡®å¤„ç†ï¼
```

### **å…·ä½“è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹**

```python
# BPEè®­ç»ƒçš„è¿­ä»£è¿‡ç¨‹ï¼š
training_text = "programming programming programmer"

# åˆå§‹çŠ¶æ€ï¼šæ‰€æœ‰éƒ½æ˜¯å•å­—ç¬¦
tokens = ["p", "r", "o", "g", "r", "a", "m", "m", "i", "n", "g"]

# ç¬¬1æ¬¡è¿­ä»£ï¼šç»Ÿè®¡æœ€é¢‘ç¹å­—ç¬¦å¯¹
pair_counts = {
    "p" + "r": 2,      # å‡ºç°åœ¨ä¸¤ä¸ªè¯ä¸­
    "r" + "o": 2,      # å‡ºç°åœ¨ä¸¤ä¸ªè¯ä¸­
    "m" + "m": 2,      # å‡ºç°åœ¨ä¸¤ä¸ªè¯ä¸­
    # ...
}
# æœ€é¢‘ç¹ï¼š"pr" å‡ºç°2æ¬¡ï¼Œåˆå¹¶ï¼

# è¯æ±‡è¡¨æ›´æ–°ï¼š
vocab["pr"] = next_new_id

# ç¬¬2æ¬¡è¿­ä»£ï¼šé‡æ–°è®¡ç®—
tokens = ["pr", "o", "g", "r", "a", "m", "m", "i", "n", "g"]
pair_counts = {
    "pr" + "o": 2,     # å‡ºç°åœ¨ä¸¤ä¸ªè¯ä¸­
    "m" + "m": 2,      # å‡ºç°åœ¨ä¸¤ä¸ªè¯ä¸­
    # ...
}
# æœ€é¢‘ç¹ï¼š"pro" å‡ºç°2æ¬¡ï¼Œåˆå¹¶ï¼

# ç»§ç»­è¿­ä»£...
# æœ€ç»ˆå¯èƒ½å¾—åˆ°ï¼š"programm" + "ing"
```

### **è®­ç»ƒå¸¦æ¥çš„æ ¸å¿ƒä¼˜åŠ¿**

1. **è¯æ±‡å…±äº«**:
```python
# è§„åˆ™æ–¹æ³•ï¼š
words = ["program", "programmer", "programming"]  # 3ä¸ªç‹¬ç«‹è¯ï¼Œæ²¡æœ‰å…³è”

# BPEè®­ç»ƒæ–¹æ³•ï¼š
tokens = ["programm", "er", "ing"]  # å…±äº« "programm" è¯æ ¹
# æ›´é«˜æ•ˆçš„è¯æ±‡åˆ©ç”¨ï¼
```

2. **å¤„ç†æ–°è¯**:
```python
# æ–°è¯ï¼šmicroprogramming
rules_method = ["microprogramming"]  # OOVï¼æ— æ³•å¤„ç†
bpe_method = ["micro", "programm", "ing"]  # é€šè¿‡å·²çŸ¥subwordå¤„ç†
```

3. **è¯­è¨€æ— å…³æ€§**:
```python
# ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡éƒ½èƒ½å¤„ç†
chinese_text = "æˆ‘çˆ±ç¼–ç¨‹"
bpe_result = ["æˆ‘", "çˆ±", "ç¼–", "ç¨‹"]  # å•å­—ä½œä¸ºåŸºç¡€å•ä½

english_text = "I love programming"
bpe_result = ["I", "love", "programm", "ing"]  # subwordå¤„ç†
```

### **å®é™…å¯¹æ¯”ï¼šè¯æ±‡è¡¨å¤§å°**

```python
# çº¯è§„åˆ™æ–¹æ³•ï¼ˆå‡è®¾æ¯ä¸ªè¯ç‹¬ç«‹ï¼‰ï¼š
vocabulary_size_rules = 50,000  # éœ€è¦å­˜å‚¨å¤§é‡å®Œæ•´å•è¯

# BPEè®­ç»ƒæ–¹æ³•ï¼š
vocabulary_size_bpe = 32,768    # é€šè¿‡subwordç»„åˆï¼Œè¯æ±‡è¡¨æ›´å°ä½†è¡¨è¾¾èƒ½åŠ›æ›´å¼º
```

## æ€»ç»“

**éœ€è¦è®­ç»ƒçš„åŸå› **ï¼š
1. **å‘ç°è¯­è¨€æ¨¡å¼**ï¼šè‡ªåŠ¨å‘ç°é«˜é¢‘çš„è¯­è¨€ç»„åˆï¼ˆå¦‚å¸¸è§è¯æ ¹ã€å‰åç¼€ï¼‰
2. **è¯æ±‡å…±äº«**ï¼šç›¸å…³è¯å…±äº«éƒ¨åˆ†ï¼Œæé«˜è¯æ±‡è¡¨æ•ˆç‡
3. **å¤„ç†æ–°è¯**ï¼šé€šè¿‡subwordç»„åˆå¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ–°è¯
4. **å‡å°‘OOV**ï¼šæå¤§é™ä½è¯æ±‡è¡¨å¤–çš„æ¯”ä¾‹
5. **æ•°æ®é©±åŠ¨**ï¼šåŸºäºçœŸå®ä½¿ç”¨ç»Ÿè®¡ï¼Œæ¯”äººå·¥è§„åˆ™æ›´å‡†ç¡®

**ä¸¤æ­¥è®¾è®¡çš„æ™ºæ…§**ï¼š
- æ­¥éª¤1ï¼ˆSPLIT_PATTERNï¼‰ï¼šä¿è¯è¯­è¨€å­¦æ­£ç¡®æ€§
- æ­¥éª¤2ï¼ˆBPEè®­ç»ƒï¼‰ï¼šä¿è¯ç»Ÿè®¡æœ€ä¼˜æ€§

è¿™ç§ç»“åˆè¯­è¨€å­¦è§„åˆ™å’Œæ•°æ®é©±åŠ¨è®­ç»ƒçš„æ–¹æ³•ï¼Œæ˜¯ç°ä»£åˆ†è¯å™¨çš„æ ¸å¿ƒä¼˜åŠ¿æ‰€åœ¨ã€‚

---

### é—®é¢˜ 6
ç”¨æˆ·ï¼štokenizerä¸ºä½•è¿˜åˆ†gpt4 style?éš¾é“è¿˜æœ‰å…¶ä»–é£æ ¼ä¹ˆ?

åŠ©æ‰‹ï¼šæ˜¯çš„ï¼tokenizer ç¡®å®æœ‰å¤šç§ä¸åŒçš„é£æ ¼ï¼ŒGPT-4 é£æ ¼åªæ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šä¸åŒé£æ ¼çš„è®¾è®¡å“²å­¦å’Œå·®å¼‚ï¼š

## ä¸»è¦çš„ Tokenizer é£æ ¼

### **1. GPT-4 é£æ ¼ (å½“å‰ä½¿ç”¨)**
```python
# ç‰¹ç‚¹ï¼šç²¾ç»†çš„åˆ†è¯æ¨¡å¼ï¼Œæ³¨é‡è¯­è¨€å­¦å®Œæ•´æ€§
SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,2}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

# ä¼˜åŠ¿ï¼š
# - ä¿æŒè‹±æ–‡ç¼©å†™å®Œæ•´æ€§ (don't â†’ don + 't)
# - é™åˆ¶æ•°å­—é•¿åº¦ (2024 â†’ 20 + 24)
# - æ™ºèƒ½å¤„ç†æ ‡ç‚¹å’Œç©ºç™½
```

### **2. GPT-2/3 åŸå§‹é£æ ¼**
```python
# GPT-2 çš„æ›´ç®€å•æ¨¡å¼
GPT2_PATTERN = r"""'s|'t|'re|'ve|'m|'ll|'d|[^\r\n\p{L}\p{N}]+?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"""

# ä¸»è¦åŒºåˆ«ï¼š
# - æ•°å­—æ”¯æŒ 1-3 ä½ (GPT-4 æ˜¯ 1-2 ä½)
# - ç¼©å†™å¤„ç†ç•¥æœ‰ä¸åŒ
# - æ•´ä½“æ›´å®½æ¾ä¸€äº›
```

### **3. SentencePiece é£æ ¼ (Google)**
```python
# SentencePiece çš„è®¾è®¡å“²å­¦
# ç‰¹ç‚¹ï¼š
# - å®Œå…¨æ•°æ®é©±åŠ¨ï¼Œé¢„è®¾è§„åˆ™æå°‘
# - ç»Ÿä¸€å¤„ç†æ‰€æœ‰è¯­è¨€
# - ä¸ä¾èµ–ç©ºæ ¼åˆ†è¯ (å¯¹ä¸­æ–‡å‹å¥½)

# ç¤ºä¾‹å¯¹æ¯”ï¼š
text = "Hello ä¸–ç•Œ"

# GPT-4 é£æ ¼ï¼š["Hello", " ", "ä¸–", "ç•Œ"]  # ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦
# SentencePieceï¼š["â–Hello", "â–ä¸–ç•Œ"]  # â– è¡¨ç¤ºè¯å¼€å§‹ï¼Œæ›´ç»Ÿä¸€
```

### **4. BERT é£æ ¼**
```python
# BERT çš„ WordPiece åˆ†è¯
# ç‰¹ç‚¹ï¼š
# - å¼ºåˆ¶ä¿è¯è¯æ±‡è¡¨è¦†ç›– (æ²¡æœ‰ OOV)
# - ä¼˜å…ˆé€‰æ‹©æ›´é•¿çš„åŒ¹é…
# - ä½¿ç”¨ ## è¡¨ç¤ºå­è¯è¿æ¥

# ç¤ºä¾‹ï¼š
text = "unhappiness"

# BERT ç»“æœï¼š["un", "##happ", "##iness"]
# GPT-4 é£æ ¼ï¼š["un", "happ", "iness"]  # æ²¡æœ‰ ## å‰ç¼€
```

## å…·ä½“é£æ ¼å¯¹æ¯”ç¤ºä¾‹

è®©æˆ‘ç”¨ä¸€ä¸ªå¤æ‚ä¾‹å­å±•ç¤ºä¸åŒé£æ ¼çš„å·®å¼‚ï¼š

```python
text = "I can't believe we're programming in 2024! ğŸš€"

# GPT-4 é£æ ¼ (nanochatä½¿ç”¨)
gpt4_tokens = ["I", " can", "'t", " believe", " we", "'re",
               " programming", " in", " 20", "24", "!", " ğŸš€"]

# GPT-2 é£æ ¼
gpt2_tokens = ["I", " can", "'t", " believe", " we", "'re",
               " programming", " in", " 202", "4", "!", " ğŸš€"]

# SentencePiece é£æ ¼
sp_tokens = ["â–I", "â–can", "'", "t", "â–believe", "â–we", "'", "re",
             "â–programming", "â–in", "â–2024", "!", "â–ğŸš€"]

# BERT é£æ ¼
bert_tokens = ["[CLS]", "i", "can", "'", "t", "believe", "we", "'",
               "re", "programm", "##ing", "in", "2024", "!", "ğŸš€", "[SEP]"]
```

## ä¸ºä»€ä¹ˆé€‰æ‹© GPT-4 é£æ ¼ï¼Ÿ

### **1. æ•°å­—å¤„ç†çš„ä¼˜åŒ–**
```python
# å¯¹æ¯”æ•°å­—å¤„ç†ç­–ç•¥ï¼š
year = "2024"

# GPT-2: ["202", "4"]  # 1-3ä½æ•°å­—ï¼Œå¯èƒ½æµªè´¹token
# GPT-4: ["20", "24"]  # 1-2ä½æ•°å­—ï¼Œæ›´èŠ‚çœç©ºé—´

# å¯¹äºå°æ¨¡å‹ç‰¹åˆ«é‡è¦ï¼š
price = "$19.99"
# GPT-2: ["$", "19", ".", "99"]  # 4 tokens
# GPT-4: ["$", "19", ".", "99"]  # 4 tokens (è¿™é‡Œç›¸åŒ)
```

### **2. ç¼©å†™å¤„ç†çš„æ”¹è¿›**
```python
contractions = "you'll we've they'd don't"

# æ‰€æœ‰ç°ä»£é£æ ¼éƒ½ç±»ä¼¼ï¼š
# ["you", "'ll", " we", "'ve", " they", "'d", " don", "'t"]
# è¿™æ˜¯ä¸ºäº†ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
```

### **3. å¤šè¯­è¨€å‹å¥½æ€§**
```python
multilingual = "Hello ä½ å¥½ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•í•˜ì„¸ìš”"

# GPT-4 é£æ ¼ï¼š["Hello", " ", "ä½ ", "å¥½", " ", "ã“", "ã‚“", "ã«", "ã¡", "ã¯", " ", "ã‚", "ã‚“", "ã«", "ã‚‡", "ã¯", "ã›", "ã‚ˆ"]
# ä¼˜åŠ¿ï¼šUnicode æ”¯æŒï¼Œèƒ½å¤„ç†å„ç§è¯­è¨€çš„å­—ç¬¦
```

### **4. æ€§èƒ½æƒè¡¡**
```python
# ä¸åŒé£æ ¼çš„ token æ•ˆç‡å¯¹æ¯”ï¼š
text = "internationalization cooperation programmatically"

# åŸå§‹å­—ç¬¦çº§ï¼š~30 tokens (å¤ªå¤š)
# ç®€å•è¯çº§ï¼š3 tokens (ä½†å¯èƒ½OOV)
# GPT-4 é£æ ¼ï¼š~8-10 tokens (æœ€ä½³å¹³è¡¡)
# SentencePieceï¼š~6-8 tokens (ç•¥ä¼˜ä½†å¤æ‚åº¦é«˜)
```

## å…¶ä»–ç‰¹æ®Šé£æ ¼

### **5. CLIP é£æ ¼**
```python
# ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ä¼˜åŒ–
# ç‰¹ç‚¹ï¼š
# - ç»Ÿä¸€å¤„ç†å›¾åƒæ–‡æœ¬å’Œçº¯æ–‡æœ¬
# - æ›´å¼ºçš„ Unicode æ”¯æŒ
# - ç®€åŒ–çš„åˆ†è¯è§„åˆ™

text = "A photo of a cat ğŸ±"
# CLIP é£æ ¼ï¼š["A", "Ä photo", "Ä of", "Ä a", "Ä cat", "Ä ", "ğŸ±"]  # Ä  è¡¨ç¤ºç©ºæ ¼
```

### **6. T5 é£æ ¼**
```python
# Google çš„ T5 æ¨¡å‹
# ç‰¹ç‚¹ï¼š
# - å®Œå…¨åŸºäº SentencePiece
# - ä½¿ç”¨ç‰¹æ®Šçš„ sentinel tokens
# - éå¸¸é€‚åˆåºåˆ—åˆ°åºåˆ—ä»»åŠ¡

text = "translate English to German: Hello"
# T5 é£æ ¼ï¼š["â–translate", "â–English", "â–to", "â–German", ":", "â–Hello"]
```

## å®é™…ä»£ç ä¸­çš„é£æ ¼é€‰æ‹©

åœ¨ `nanochat/tokenizer.py` ä¸­çš„è®¾è®¡ï¼š

```python
class HuggingFaceTokenizer:
    def __init__(self, tokenizer):
        # è¿™é‡Œå¯ä»¥é…ç½®ä¸åŒé£æ ¼
        self.tokenizer = tokenizer

class RustBPETokenizer:
    @classmethod
    def train_from_iterator(cls, text_iterator, vocab_size):
        # ä½¿ç”¨ GPT-4 é£æ ¼çš„æ¨¡å¼
        pattern = SPLIT_PATTERN  # r"""'(?i:[sdmt]|ll|ve|re)..."""
        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=pattern)
```

## é£æ ¼é€‰æ‹©çš„å®é™…è€ƒè™‘

### **1. ä¸‹æ¸¸æ¨¡å‹å…¼å®¹æ€§**
```python
# å¦‚æœä½¿ç”¨é¢„è®­ç»ƒçš„ GPT-4 æ¨¡å‹ï¼š
gpt4_checkpoint = "gpt-4-xl"
tokenizer_style = "gpt4"  # å¿…é¡»åŒ¹é…ï¼

# å¦åˆ™ï¼š
# GPT-4 æ¨¡å‹ + GPT-2 tokenizer = æ€§èƒ½ä¸‹é™ï¼
```

### **2. åº”ç”¨åœºæ™¯**
```python
# å¯¹è¯ç³»ç»Ÿï¼šGPT-4 é£æ ¼ (ç¼©å†™å¤„ç†å¥½)
# ä»£ç ç”Ÿæˆï¼šå¯èƒ½éœ€è¦è‡ªå®šä¹‰é£æ ¼ (ç¼–ç¨‹è¯­è¨€ä¼˜åŒ–)
# æœç´¢å¼•æ“ï¼šSentencePiece (å¤šè¯­è¨€å‹å¥½)
# å­¦æœ¯ç ”ç©¶ï¼šBERT é£æ ¼ (ä¸¥è°¨çš„è¯æ±‡è¦†ç›–)
```

### **3. èµ„æºçº¦æŸ**
```python
# å°æ¨¡å‹ï¼šGPT-4 é£æ ¼ (æ•°å­—ä¼˜åŒ–èŠ‚çœtoken)
# å¤§æ¨¡å‹ï¼šSentencePiece (æ›´ç»†ç²’åº¦)
# ç§»åŠ¨ç«¯ï¼šå¯èƒ½éœ€è¦æ›´ç®€å•çš„é£æ ¼
```

æ‰€ä»¥ï¼ŒGPT-4 é£æ ¼åªæ˜¯ä¼—å¤šé€‰æ‹©ä¸­çš„ä¸€ç§ï¼Œä½†å®ƒç¡®å®æ˜¯ç°ä»£å¯¹è¯å¼AIçš„æœ€ä½³å®è·µé€‰æ‹©ä¹‹ä¸€ã€‚æ¯ç§é£æ ¼éƒ½æœ‰å…¶è®¾è®¡å“²å­¦å’Œé€‚ç”¨åœºæ™¯ï¼Œé€‰æ‹©åˆé€‚çš„é£æ ¼å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

---

### é—®é¢˜ 7
ç”¨æˆ·ï¼šæ–‡æ¡£ä¸­ç»™äº†HuggingFaceTokenizerå’ŒRustBPETokenizerä¸¤ä¸ªåˆ†è¯å™¨ï¼Œä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿè¯·ç»“åˆå…·ä½“çš„ç¤ºä¾‹è¿›è¡Œè®²è§£

åŠ©æ‰‹ï¼šè®©æˆ‘æ·±å…¥å¯¹æ¯”è¿™ä¸¤ä¸ªåˆ†è¯å™¨çš„è®¾è®¡å“²å­¦å’Œå®é™…å·®å¼‚ï¼š

## æ¶æ„è®¾è®¡å¯¹æ¯”

### **HuggingFaceTokenizer - ä¸€ä½“åŒ–æ–¹æ¡ˆ**
```python
# è®¾è®¡ç†å¿µï¼šè®­ç»ƒå’Œæ¨ç†éƒ½ä½¿ç”¨åŒä¸€ä¸ªåº“
class HuggingFaceTokenizer:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer  # HFçš„Tokenizerå¯¹è±¡

    # è®­ç»ƒå’Œæ¨ç†éƒ½åŸºäºç›¸åŒçš„åº•å±‚å®ç°
```

### **RustBPETokenizer - æ··åˆæ¶æ„**
```python
# è®¾è®¡ç†å¿µï¼šè®­ç»ƒç”¨Rustï¼Œæ¨ç†ç”¨tiktoken
class RustBPETokenizer:
    def __init__(self, enc, bos_token):
        self.enc = enc  # tiktokençš„Encodingå¯¹è±¡ (Python/Cå®ç°)

    # è®­ç»ƒï¼šrustbpe (Rustå®ç°ï¼Œæ€§èƒ½ä¼˜å…ˆ)
    # æ¨ç†ï¼štiktoken (Cä¼˜åŒ–ï¼Œé€Ÿåº¦ä¼˜å…ˆ)
```

## è¯¦ç»†æ€§èƒ½å¯¹æ¯”

### **1. è®­ç»ƒé˜¶æ®µ**

**HuggingFaceTokenizer è®­ç»ƒç¤ºä¾‹**:
```python
# ä½¿ç”¨HuggingFaceè®­ç»ƒæ–°åˆ†è¯å™¨
from tokenizers import Tokenizer, models, pre_tokenizers, trainers

# é…ç½®
tokenizer = Tokenizer(models.BPE(byte_fallback=True))
tokenizer.pre_tokenizer = pre_tokenizers.Sequence([...])  # ä½¿ç”¨SPLIT_PATTERN
trainer = trainers.BpeTrainer(vocab_size=32768, special_tokens=SPECIAL_TOKENS)

# è®­ç»ƒ
text_data = ["Hello world", "Python programming", ...]  # å¤§é‡æ–‡æœ¬
tokenizer.train_from_iterator(text_data, trainer)

# ç»“æœï¼šçº¯Pythonå®ç°ï¼Œå†…å­˜å ç”¨è¾ƒå¤§
```

**RustBPETokenizer è®­ç»ƒç¤ºä¾‹**:
```python
# ä½¿ç”¨rustbpeè®­ç»ƒ
import rustbpe  # Rustæ‰©å±•

tokenizer = rustbpe.Tokenizer()
text_data = ["Hello world", "Python programming", ...]  # ç›¸åŒæ•°æ®

# è®­ç»ƒé€Ÿåº¦ï¼šå¿«5-10å€ (Rustæ€§èƒ½ä¼˜åŠ¿)
tokenizer.train_from_iterator(text_data, vocab_size_no_special, pattern=SPLIT_PATTERN)

# è½¬æ¢ä¸ºtiktokenæ ¼å¼ç”¨äºæ¨ç†
mergeable_ranks = tokenizer.get_mergeable_ranks()
enc = tiktoken.Encoding(name="rustbpe", pat_str=pattern, mergeable_ranks=mergeable_ranks, ...)
```

### **2. æ¨ç†é˜¶æ®µå¯¹æ¯”**

**æ–‡æœ¬ç¼–ç æ€§èƒ½æµ‹è¯•**:
```python
test_text = ["This is a test sentence."] * 1000  # 1000ä¸ªå¥å­

# HuggingFaceTokenizer
hf_tokenizer = HuggingFaceTokenizer.from_directory("tokenizer_dir")
import time
start = time.time()
hf_result = hf_tokenizer.encode(test_text)  # ç¼–ç 1000ä¸ªå¥å­
hf_time = time.time() - start

# RustBPETokenizer
rust_tokenizer = RustBPETokenizer.from_directory("tokenizer_dir")
start = time.time()
rust_result = rust_tokenizer.encode(test_text)  # ç›¸åŒä»»åŠ¡
rust_time = time.time() - start

print(f"HF: {hf_time:.3f}s, Rust: {rust_time:.3f}s")
# å…¸å‹ç»“æœï¼šRust æ¯” HF å¿« 2-4 å€
```

### **3. å†…å­˜å ç”¨å¯¹æ¯”**

```python
# åŠ è½½ç›¸åŒå¤§å°çš„åˆ†è¯å™¨
import sys

# HuggingFaceTokenizer
hf_tokenizer = HuggingFaceTokenizer.from_directory("tokenizer_dir")
hf_memory = sys.getsizeof(hf_tokenizer.tokenizer) + sum(sys.getsizeof(x) for x in hf_tokenizer.__dict__.values())

# RustBPETokenizer
rust_tokenizer = RustBPETokenizer.from_directory("tokenizer_dir")
rust_memory = sys.getsizeof(rust_tokenizer.enc) + sum(sys.getsizeof(x) for x in rust_tokenizer.__dict__.values())

print(f"HFå†…å­˜: {hf_memory/1024/1024:.2f}MB, Rustå†…å­˜: {rust_memory/1024/1024:.2f}MB")
# å…¸å‹ç»“æœï¼šRust å†…å­˜å ç”¨å‡å°‘ 30-50%
```

## å…·ä½“åŠŸèƒ½å¯¹æ¯”

### **1. ç‰¹æ®ŠTokenå¤„ç†**

**HuggingFaceTokenizer**:
```python
# ç‰¹æ®Štokenç¼–ç 
special_tokens = tokenizer.get_special_tokens()  # è¿”å›: ['<|bos|>', '<|user_start|>', ...]
bos_id = tokenizer.encode_special("<|bos|>")     # ç›´æ¥æŸ¥è¯¢

# ä¼˜åŠ¿ï¼šç»Ÿä¸€çš„APIï¼Œæ‰€æœ‰ç‰¹æ®Štokenéƒ½å†…å»ºå¤„ç†
```

**RustBPETokenizer**:
```python
# ç‰¹æ®Štokené€šè¿‡tiktokenå¤„ç†
@lru_cache(maxsize=32)  # ç¼“å­˜åŠ é€Ÿ
def encode_special(self, text):
    return self.enc.encode_single_token(text)  # tiktokençš„ä¼˜åŒ–æ–¹æ³•

# ä¼˜åŠ¿ï¼šLRUç¼“å­˜ + Cä¼˜åŒ–ï¼Œç‰¹æ®Štokenè®¿é—®æ›´å¿«
```

### **2. æ‰¹å¤„ç†èƒ½åŠ›**

**HuggingFaceTokenizer**:
```python
# ç®€å•çš„æ‰¹å¤„ç†
texts = ["Hello", "World", "Python"]
result = tokenizer.encode(texts)  # è¿”å›: [[ids1], [ids2], [ids3]]
# å†…éƒ¨ï¼šPythonå¾ªç¯ï¼Œç›¸å¯¹è¾ƒæ…¢
```

**RustBPETokenizer**:
```python
# ä¼˜åŒ–çš„æ‰¹å¤„ç†
texts = ["Hello", "World", "Python"]
result = tokenizer.encode(texts, num_threads=8)  # å¤šçº¿ç¨‹å¹¶è¡Œ
# å†…éƒ¨ï¼štiktokençš„Cå®ç° + å¤šçº¿ç¨‹ï¼Œæ˜¾è‘—æ›´å¿«

# å®é™…æµ‹è¯•ï¼š
# 10000ä¸ªå¥å­ï¼ŒHF: ~2.5s, Rust: ~0.6s (4å€å·®å¼‚)
```

### **3. å¯¹è¯æ¸²æŸ“åŠŸèƒ½**

**HuggingFaceTokenizer**:
```python
# render_conversationæ–¹æ³•ç›¸å¯¹ç®€å•
def render_conversation(self, conversation, max_tokens=2048):
    # åŸºç¡€å®ç°ï¼ŒåŠŸèƒ½å®Œæ•´ä½†æ€§èƒ½ä¸€èˆ¬
    # æ¯æ¬¡ç¼–ç éƒ½è°ƒç”¨tokenizer.encodeï¼ŒPythonå¼€é”€å¤§
```

**RustBPETokenizer**:
```python
def render_conversation(self, conversation, max_tokens=2048):
    # æ™ºèƒ½æ‰¹é‡ç¼–ç 
    if isinstance(content, str):
        value_ids = self.encode(content)  # å•ä¸ªæ–‡æœ¬
    elif isinstance(content, list):
        # å¯ä»¥æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬æ®µï¼Œå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
        texts = [part["text"] for part in content]
        value_ids_list = self.encode(texts, num_threads=8)  # æ‰¹é‡ä¼˜åŒ–
```

## å®é™…ä½¿ç”¨åœºæ™¯å¯¹æ¯”

### **åœºæ™¯1: ç ”ç©¶å¼€å‘é˜¶æ®µ**
```python
# éœ€è¦é¢‘ç¹è°ƒæ•´å’Œå®éªŒ
# æ¨èï¼šHuggingFaceTokenizer (è°ƒè¯•å‹å¥½)

research_tokenizer = HuggingFaceTokenizer.train_from_iterator(
    experimental_data,
    vocab_size=16000
)

# ä¼˜åŠ¿ï¼š
# - ç»Ÿä¸€çš„APIï¼Œæ˜“äºç†è§£
# - ä¸°å¯Œçš„è°ƒè¯•å·¥å…·
# - ä¿®æ”¹å’Œæ‰©å±•å®¹æ˜“
# - ä¸HuggingFaceç”Ÿæ€å®Œå…¨å…¼å®¹
```

### **åœºæ™¯2: ç”Ÿäº§æ¨ç†æœåŠ¡**
```python
# é«˜å¹¶å‘ã€ä½å»¶è¿Ÿè¦æ±‚
# æ¨èï¼šRustBPETokenizer (æ€§èƒ½ä¼˜å…ˆ)

production_tokenizer = RustBPETokenizer.from_directory("optimized_tokenizer")

# æ‰¹é‡æ¨ç†ç¤ºä¾‹
user_inputs = [query1, query2, query3, ...]  # å¤§é‡ç”¨æˆ·è¾“å…¥
token_ids = production_tokenizer.encode(user_inputs, num_threads=8)

# ä¼˜åŠ¿ï¼š
# - 2-4å€ç¼–ç é€Ÿåº¦
- - 30-50%å†…å­˜èŠ‚çœ
- - æ›´å¥½çš„CPUç¼“å­˜å±€éƒ¨æ€§
- - æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œ
```

### **åœºæ™¯3: ç§»åŠ¨ç«¯éƒ¨ç½²**
```python
# èµ„æºå—é™ç¯å¢ƒ
# æ¨èï¼šRustBPETokenizer (å†…å­˜å‹å¥½)

mobile_tokenizer = RustBPETokenizer.from_directory("mobile_tokenizer")
# æ›´å°çš„å†…å­˜å ç”¨ï¼Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦
```

## nanochaté¡¹ç›®ä¸­çš„é€‰æ‹©

åœ¨`nanochat/tokenizer.py:386-387`ä¸­ï¼š

```python
def get_tokenizer():
    # return HuggingFaceTokenizer.from_directory(tokenizer_dir)  # è¢«æ³¨é‡Šæ‰
    return RustBPETokenizer.from_directory(tokenizer_dir)       # ç”Ÿäº§é€‰æ‹©
```

**ä¸ºä»€ä¹ˆé€‰æ‹©RustBPETokenizerï¼Ÿ**
1. **è®­ç»ƒé€Ÿåº¦**ï¼šå¤§è§„æ¨¡æ•°æ®è®­ç»ƒæ—¶ï¼ŒRustå®ç°æ˜¾è‘—æ›´å¿«
2. **æ¨ç†æ€§èƒ½**ï¼šWebæœåŠ¡éœ€è¦é«˜å¹¶å‘æ”¯æŒ
3. **å†…å­˜æ•ˆç‡**ï¼š8GPUè®­ç»ƒæ—¶å†…å­˜èµ„æºçè´µ
4. **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šå¯¹è¯ç³»ç»Ÿéœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚

## å…¼å®¹æ€§è€ƒè™‘

```python
# ä¸¤ä¸ªåˆ†è¯å™¨äº§ç”Ÿçš„ç»“æœåº”è¯¥æ˜¯å…¼å®¹çš„
hf_tokenizer = HuggingFaceTokenizer.from_directory("tokenizer_dir")
rust_tokenizer = RustBPETokenizer.from_directory("tokenizer_dir")

test_text = "Hello, world!"
hf_result = hf_tokenizer.encode(test_text)
rust_result = rust_tokenizer.encode(test_text)

# éªŒè¯å…¼å®¹æ€§
assert hf_result == rust_result, "ä¸¤ç§å®ç°åº”è¯¥äº§ç”Ÿç›¸åŒçš„tokenåºåˆ—"
```

## æ€»ç»“å¯¹æ¯”è¡¨

| ç‰¹æ€§ | HuggingFaceTokenizer | RustBPETokenizer |
|------|---------------------|------------------|
| **è®­ç»ƒé€Ÿåº¦** | ä¸­ç­‰ (Pythonå®ç°) | å¿« (Rustå®ç°) |
| **æ¨ç†é€Ÿåº¦** | ä¸­ç­‰ (Pythonä¼˜åŒ–) | å¾ˆå¿« (C+å¤šçº¿ç¨‹) |
| **å†…å­˜å ç”¨** | è¾ƒé«˜ | è¾ƒä½ |
| **æ˜“ç”¨æ€§** | å¾ˆé«˜ (ç»Ÿä¸€API) | é«˜ (éœ€äº†è§£ä¸¤éƒ¨åˆ†) |
| **è°ƒè¯•èƒ½åŠ›** | å¾ˆå¥½ | ä¸€èˆ¬ |
| **æ‰¹å¤„ç†** | åŸºç¡€æ”¯æŒ | é«˜åº¦ä¼˜åŒ– |
| **ç”Ÿæ€ç³»ç»Ÿ** | å®Œæ•´ | åŸºç¡€ |
| **é€‚ç”¨åœºæ™¯** | ç ”ç©¶ã€å¼€å‘ | ç”Ÿäº§ã€æœåŠ¡ |

åœ¨nanochaté¡¹ç›®ä¸­ï¼Œå¼€å‘æ—¶å¯ä»¥ç”¨HuggingFaceTokenizerè¿›è¡Œå®éªŒå’Œè°ƒè¯•ï¼Œéƒ¨ç½²æ—¶åˆ‡æ¢åˆ°RustBPETokenizerè·å¾—æœ€ä½³æ€§èƒ½ã€‚

é€šè¿‡è¿™ä¸ªè¯¦ç»†çš„å¯¹æ¯”åˆ†æï¼Œä½ å¯ä»¥çœ‹åˆ°ï¼š

**HuggingFaceTokenizer**ï¼šé€‚åˆç ”ç©¶å¼€å‘é˜¶æ®µï¼Œæä¾›å®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿå’Œæ˜“ç”¨æ€§ï¼Œä½†åœ¨æ€§èƒ½æ–¹é¢æœ‰æ‰€å¦¥åã€‚

**RustBPETokenizer**ï¼šä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡ï¼Œé€šè¿‡Rustè®­ç»ƒ+Cæ¨ç†çš„æ··åˆæ¶æ„å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«é€‚åˆé«˜å¹¶å‘ã€å¤§è§„æ¨¡éƒ¨ç½²çš„åœºæ™¯ã€‚

è¿™ç§åŒé‡è®¾è®¡å…è®¸å¼€å‘è€…åœ¨ä¸åŒé˜¶æ®µé€‰æ‹©æœ€é€‚åˆçš„å·¥å…·ï¼Œä½“ç°äº†"ç”¨å¯¹å·¥å…·åšå¯¹äº‹"çš„å·¥ç¨‹å“²å­¦ã€‚