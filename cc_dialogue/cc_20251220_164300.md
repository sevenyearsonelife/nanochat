# Chat Dialogue Log

## 对话时间
2025-12-20 16:43:00

---

## 对话内容

### 问题 1
用户：@scripts/mid_train.py @scripts/chat_sft.py 中训练和SFT的主要区别是什么？请结合代码解释
助手：在 `nanochat` 项目中，`mid_train.py`（Mid-Training / 继续预训练）与 `chat_sft.py`（Supervised Fine-Tuning / 指令微调）的核心区别在于**数据处理方式**、**损失计算掩码**以及**训练目标**。

以下是结合代码的详细对比分析：

### 1. 数据处理与掩码策略 (主要区别)

**Mid-Training (`scripts/mid_train.py`)**
采用**流式拼接**的方式。它将所有对话数据拼接成一长串 token流，**不区分**用户提问和助手回答，对所有 token 进行预测训练。

*   **代码证据 (Lines 145-150):**
    可以看到输入和目标仅仅是错一位的关系（Next Token Prediction），没有应用任何掩码（Mask）。
    ```python:scripts/mid_train.py
    # ...
    inputs_cpu = scratch[:-1].to(dtype=torch.int32)
    targets_cpu = scratch[1:] # 目标就是下一个token，没有任何 -1 掩码
    inputs = inputs_cpu.view(device_batch_size, max_seq_len).to(device=device, dtype=torch.int32, non_blocking=True)
    targets = targets_cpu.view(device_batch_size, max_seq_len).to(device=device, dtype=torch.int64, non_blocking=True)
    ```

**SFT (`scripts/chat_sft.py`)**
采用**基于掩码的微调**。它明确区分 User 和 Assistant 的部分，**只计算 Assistant 回答部分的 Loss**，将 User 的提示部分（Prompt）在 Loss 计算中掩盖掉（设为 ignore_index -1）。

*   **代码证据 (Lines 111-114):**
    利用 tokenizer 返回的 `mask`，将不需要计算 loss 的位置（如 User 的发言）在 `targets` 中设为 `-1`。
    ```python:scripts/chat_sft.py
    # ...
    row_targets = ids_tensor[1:]
    # mask[1:] omits the mask for the BOS token...
    mask_tensor = torch.tensor(mask[1:], dtype=torch.long)
    row_targets[mask_tensor == 0] = -1 # <--- 核心区别：将 mask 为 0 的位置设为 -1 (ignore_index)
    targets[i, :n-1] = row_targets
    ```

### 2. Batch 组装方式

*   **Mid-Training:** 追求高吞吐。使用 `deque` 缓存 token，填满 `device_batch_size * max_seq_len` 就生成一个 batch，跨越文档边界（Lines 124-140）。
*   **SFT:** 追求样本独立性。每个样本是一个独立的对话，长度不足时填充 `pad_token_id`（这里复用了 `<|assistant_end|>`），并对 Padding 部分也进行 Mask（Lines 99-105）。

### 3. 优化器与调度 (Optimization)

*   **Mid-Training:** 使用了更复杂的优化策略，包括针对矩阵乘法层的 **Muon 优化器** 和针对其他层的 AdamW，以及自定义的动量调度（Lines 168-171）。
*   **SFT:** 使用标准的优化器设置，学习率调度也是简单的线性衰减（Lines 164-166），且学习率通常比 Mid-Training 低（`init_lr_frac` 为 0.02 vs Mid 的 1.0）。

### 4. 评估指标

*   **Mid-Training:** 关注 **BPB (Bits Per Byte)** 和困惑度，主要看模型对语言本身的建模能力（Lines 196-197）。
*   **SFT:** 除了 Loss，还引入了下游任务的实测，如 **MMLU** 和 **ARC** 的准确率（Lines 201-202），以确保微调后的模型具备指令遵循和推理能力。

### 总结

| 特性 | Mid-Training (`mid_train.py`) | SFT (`chat_sft.py`) |
| :--- | :--- | :--- |
| **目标** | 注入领域知识，适应对话格式 | 学习指令遵循，对齐人类偏好 |
| **Loss 计算** | 预测**所有** Token (全量自回归) | **只预测** Assistant 的回答 (Mask User Prompt) |
| **数据流** | 连续 Token 流 (Packed) | 独立样本 + Padding |
| **优化器** | Muon + AdamW (追求大规模训练效率) | 标准优化器配置 |
| **输入模型** | Base Model | Mid-Trained Model (通常) |
