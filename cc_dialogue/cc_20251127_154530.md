# Chat Dialogue Log

## 对话时间
2025-11-27 15:45:30

---

## 对话内容

### 问题 1
用户：@nanochat/gpt.py 代码中给出了GPT模型的代码，请绘制出ASCII文本结构图！

助手：根据 `nanochat/gpt.py` 的代码分析，这个 GPT 模型具有一些独特的现代特性（如 Muon 优化器兼容设计、QK Norm、ReLU² 激活等）。以下是其架构的 ASCII 文本结构图：

### NanoChat GPT 架构图

```text
                               Input Indices (B, T)
                                       |
                       +-------------------------------+
                       |  wte: Embedding(vocab, dim)   |
                       +---------------+---------------+
                                       |
                                   [RMSNorm] (无参数)
                                       |
            +==========================v==========================+
            |                                                     |
            |              Transformer Block (x Layers)           |
            |                                                     |
            |           Input x                                   |
            |             |                                       |
            |      +------+-------+                               |
            |      |              |                               |
            |  [RMSNorm]          | (Residual)                    |
            |      |              |                               |
            |      v              |                               |
            |  +-------+          |                               |
            |  |  Attn |          |                               |
            |  +---+---+          |                               |
            |      |              |                               |
            |      +<-------------+                               |
            |      |                                              |
            |      x                                              |
            |      |                                              |
            |      +------+-------+                               |
            |      |              |                               |
            |  [RMSNorm]          | (Residual)                    |
            |      |              |                               |
            |      v              |                               |
            |  +-------+          |                               |
            |  |  MLP  |          |                               |
            |  +---+---+          |                               |
            |      |              |                               |
            |      +<-------------+                               |
            |      |                                              |
            |    Output                                           |
            |                                                     |
            +==========================+==========================+
                                       |
                                   [RMSNorm] (Final Norm, 无参数)
                                       |
                       +---------------+---------------+
                       |   lm_head: Linear(dim, vocab) | (无 Bias)
                       +---------------+---------------+
                                       |
                       +---------------+---------------+
                       |   Logits Softcap (tanh * 15)  |
                       +---------------+---------------+
                                       |
                               Output Logits / Loss
```

### 详细组件结构

#### 1. Attention (CausalSelfAttention)
特点：RoPE, QK Norm, MQA/GQA 支持

```text
               Input (Batch, Seq, Dim)
                         |
      +-----------+------+------+-----------+
      |           |             |           |
  c_q (Linear)  c_k (Linear)  c_v (Linear)  |
      |           |             |           |
      v           v             v           |
    [Q]         [K]           [V]           |
      |           |             |           |
  Apply RoPE  Apply RoPE        |           |
      |           |             |           |
  [RMSNorm]   [RMSNorm]         |           | (QK Norm)
      |           |             |           |
      +-----+-----+             |           |
            |                   |           |
            v                   |           |
     Scaled Dot Product <-------+           |
     Attention (Causal)                     |
            |                               |
            v                               |
      c_proj (Linear)                       |
            |                               |
          Output ---------------------------+ (注意：残差在Block层级)
```

#### 2. MLP (Multi-Layer Perceptron)
特点：ReLU Squared 激活, 4x 扩展

```text
               Input (Batch, Seq, Dim)
                         |
                c_fc (Linear, 4x Dim)
                         |
                         v
                    ReLU(x)^2     <-- (ReLU Squared Activation)
                         |
                         v
               c_proj (Linear, 1x Dim)
                         |
                       Output
```

### 关键特性总结
1.  **Pre-Norm 架构**：Norm 在 Attention 和 MLP 之前应用，且在 Embedding 后和输出前也有 Norm。
2.  **无参数 RMSNorm**：使用 `F.rms_norm` 但没有可学习的 affine 参数（weight/bias）。
3.  **Rotary Embeddings (RoPE)**：无绝对位置编码，在 Attention 内部应用旋转。
4.  **QK Norm**：Query 和 Key 在做 Attention 之前先经过 Norm，稳定训练。
5.  **Logits Softcap**：输出 Logits 经过 `30 * tanh(x/30)` (代码中是15)，防止 Logits 爆炸。
6.  **无 Bias**：所有 Linear 层均没有 Bias (`bias=False`)。

---

### 问题 2
用户：GPT模型如何初始化？其输入是什么？给出具体的代码示例

助手：根据 `nanochat/gpt.py` 的代码，GPT 模型的初始化和使用流程如下：

### 1. 初始化 (Initialization)
模型初始化需要两个步骤：
1.  **配置**：实例化 `GPTConfig` 数据类，定义模型规模（层数、头数、嵌入维度等）。
2.  **构建与权重初始化**：实例化 `GPT` 类，并**显式调用** `init_weights()` 方法。该方法包含特定的初始化逻辑（如将输出层 `lm_head` 权重置零，以确保初始训练更加稳定）。

### 2. 输入 (Input)
模型的 `forward` 方法主要接收以下参数：
*   `idx` (必须): 形状为 `(Batch_Size, Sequence_Length)` 的 `LongTensor`，包含词表索引（token IDs）。
*   `targets` (可选): 形状同 `idx`，用于训练时计算交叉熵损失。
*   `kv_cache` (可选): 用于推理时的 Key-Value 缓存对象。

### 3. 代码示例

```python
import torch
from nanochat.gpt import GPT, GPTConfig

# 1. 定义配置 (这里使用一个小模型作为演示)
config = GPTConfig(
    vocab_size=50304, # 词表大小
    sequence_len=1024,# 最大上下文长度
    n_layer=4,        # Transformer 层数
    n_head=4,         # 注意力头数
    n_embd=128        # 嵌入维度
)

# 2. 实例化模型
model = GPT(config)

# 3. 初始化权重 (关键步骤)
# 应用特定的正态分布初始化，并将 lm_head 和部分投影层置零
model.init_weights()

print(f"模型参数量: {sum(p.numel() for p in model.parameters())}")

# --- 场景 A: 推理 (Inference) ---
# 构造模拟输入: Batch Size=2, Sequence Length=16
B, T = 2, 16
idx = torch.randint(0, config.vocab_size, (B, T))

# 前向传播 (不传 targets)
logits = model(idx)

# 输出形状: (Batch, Seq_Len, Vocab_Size)
print(f"\n输入形状: {idx.shape}")
print(f"Logits 形状: {logits.shape}") # -> torch.Size([2, 16, 50304])


# --- 场景 B: 训练 (Training) ---
# 构造模拟目标 (通常是 idx 向后移一位)
targets = torch.randint(0, config.vocab_size, (B, T))

# 前向传播 (传入 targets)
loss = model(idx, targets=targets)

# 输出是一个标量 Loss
print(f"\nTraining Loss: {loss.item():.4f}")
```

### 关键细节
*   **Logits Softcap**: 模型输出 logits 经过了 `15 * tanh(logits / 15)` 的处理（见 `gpt.py` 第 268 行），这限制了 logits 的数值范围，有助于训练稳定性，但这不影响输入输出的形状。
*   **Rotary Embeddings**: 初始化时会自动根据 `sequence_len` 预计算旋转位置编码（RoPE），无需用户手动传入位置信息。

---

### 问题 3
用户：（用户遇到了 AssertionError 错误，是因为 `n_kv_head` 配置不正确导致的）

助手：这个错误是因为 `GPTConfig` 中 `n_kv_head`（KV Cache 头数）的默认值是 **6**，而你设置的 `n_head`（Query 头数）是 **4**。这违反了 GQA/MQA 的约束条件：
1.  `n_kv_head` 必须小于或等于 `n_head`。
2.  `n_head` 必须能被 `n_kv_head` 整除。

在你的配置中：`6 <= 4` 为 False，触发了 `AssertionError`。

### 修复方法

需要在初始化 `GPTConfig` 时显式设置 `n_kv_head`。对于小模型，通常设置 `n_kv_head` 等于 `n_head`（标准多头注意力）或设为 1（Multi-Query Attention）。

请修改你的配置代码如下：

```python
config = GPTConfig(
    vocab_size=50304,
    sequence_len=1024,
    n_layer=4,
    n_head=4,
    n_kv_head=4,      # <--- 新增这一行，显式设为 4 (或者 1/2)
    n_embd=128
)

# 再次运行
model = GPT(config)
model.init_weights()
print("模型初始化成功！")
```

---

### 问题 4
用户：模型的入参idx是什么？

助手：`idx` 是模型的输入张量，表示**Token 索引序列**。

### 核心属性
*   **类型**: `torch.LongTensor` (长整型张量)
*   **形状**: `(B, T)`
    *   `B` (Batch Size): 批次大小，即一次并行处理的样本数量。
    *   `T` (Time / Sequence Length): 序列长度，即每个样本包含的 Token 数量。
*   **取值范围**: `[0, vocab_size - 1]`，每个整数对应词表中的一个 Token。

### 举例说明
假设词表大小为 50304，你想输入两句话：
1.  "Hello world" (假设 Token ID 为 `[15496, 995]`)
2.  "Good morning" (假设 Token ID 为 `[5891, 3381]`)

那么 `idx` 就是一个 `(2, 2)` 的张量：
```python
idx = torch.tensor([
    [15496, 995],  # 第一句话
    [5891, 3381]   # 第二句话
], dtype=torch.long)
```

---

### 问题 5
用户：我感觉gpt.py这份代码有点复杂，请问我该如何彻底掌握它？

助手：这份 `gpt.py` 代码虽然只有 300 行左右，但它通过**极简风格**实现了当今最先进的大模型架构（类似 Llama 3 / Mistral 的变体）。

要彻底掌握它，建议采用**"由外入内，动态调试"**的四步学习法。不要试图一次读懂所有细节，而是分模块击破。

### 第一阶段：宏观数据流（The Shape Game）

不要纠结数学公式，先看**张量形状（Shape）的变化**。在 `GPT.forward` 中插入 print 语句，或者使用调试器。

**核心关注点：**
1.  `idx` (B, T) 进，`logits` (B, T, V) 出。
2.  **维度变换**：输入是 (Batch, Time)，进入 Attention 后变成了 (Batch, Head, Time, Head_Dim)。这个 `transpose` 操作是理解多头注意力的关键。

### 第二阶段：核心组件拆解

这份代码有三个"心脏"部件，按难度排序攻略：

#### 1. MLP (最简单)
*   **代码**: `class MLP`
*   **逻辑**: 它是"升维再降维"。输入 `dim` -> 放大 4 倍 -> 激活 -> 缩小回 `dim`。
*   **特色**: 注意 `x = F.relu(x).square()`。标准的 GPT-2 用的是 GELU，这里用了 ReLU 的平方（近似 SwiGLU 的效果但更省计算），这是现代 LLM 的常见 Trick。

#### 2. RoPE 旋转位置编码 (中等)
*   **代码**: `apply_rotary_emb` 和 `_precompute_rotary_embeddings`
*   **难点**: 它不加在 Embedding 上，而是在 Attention 内部旋转 Q 和 K。
*   **如何掌握**:
    *   不用深究复数旋转的数学推导。
    *   只需明白：它把向量两两分组，转了一个角度。
    *   **关键行**: `x1 * cos + x2 * sin`。它让模型知道"Token A 在 Token B 前面 3 格"，而不是"Token A 在第 5 格"。

#### 3. CausalSelfAttention (最难 - 大Boss)
*   **代码**: `class CausalSelfAttention`
*   **难点**: 它混合了**训练模式**和**推理模式 (KV Cache)**。
*   **拆解阅读**:
    *   **Q/K/V 投影**: `self.c_q`, `self.c_k`, `self.c_v`。注意 `n_kv_head`，这是 GQA/MQA 的实现（K/V 头数少于 Q 头数，为了省显存）。
    *   **KV Cache (推理专用)**: 看 `if kv_cache is not None:` 分支。这是让 ChatGPT 能快速吐字的关键，它记住了之前的计算结果。
    *   **Flash Attention**: 看 `F.scaled_dot_product_attention`。代码没有手写 Softmax 公式，而是直接调优化的核函数。

### 第三阶段：训练 vs 推理 (The Dual Path)

这份代码最精妙的地方在于它把训练和推理写在了一起。

*   **训练 (Training)**:
    *   没有 `kv_cache`。
    *   一次性并行计算所有 Token。
    *   Mask 矩阵是**下三角矩阵**（不能看后面）。
*   **推理 (Inference / Generate)**:
    *   有 `kv_cache`。
    *   **逐个 Token** 串行计算。
    *   每次只算**最后一个** Token 的 Q，去查以前缓存的 K, V。

### 第四阶段：魔鬼细节 (Modern Tricks)

当你能跑通上面的流程后，最后关注这些提升效果的细节：

1.  **RMSNorm (无参数)**: `def norm(x)`。它没有 weight 和 bias，只做缩放。比 LayerNorm 简单且效果好。
2.  **QK Norm**: 在 Attention 里，Q 和 K 在点积前先做了一次 `norm(q)`。这是 Google/DeepMind 发现的稳定训练的神技，防止注意力分数爆炸。
3.  **Logits Softcap**: `logits = softcap * torch.tanh(logits / softcap)`。这限制了输出的最大值，防止 Loss 变成 NaN。

### 总结：你的学习路线图

建议你按照这个顺序新建一个 `learn_gpt.ipynb` 进行实验：

1.  **Level 1**: 实例化 `GPTConfig` 和 `GPT`，打印 `model` 结构字符串。
2.  **Level 2**: 构造随机输入 `idx`，跑通 `model(idx)`，确保不报错。
3.  **Level 3 (关键)**: 复制 `CausalSelfAttention` 类到 Notebook 里，手动输入 Q, K, V，观察 `apply_rotary_emb` 对数值的改变。
4.  **Level 4**: 跑一下 `generate` 函数，打印每一步生成的 Token。

当你能解释清楚**"为什么推理时 K 和 V 不需要重新计算，而 Q 每次都是新的？"**这个问题时，你就彻底掌握了这份代码。

---

### 问题 6
用户：@nanochat/gpt.py 这份代码的训练和推理分别是？

助手：在 `nanochat/gpt.py` 中，训练（Training）和推理（Inference）通过同一个 `forward` 函数的不同参数分支以及独立的 `generate` 函数来区分实现。

### 1. 训练 (Training)

**入口**：`forward` 方法，当传入 `targets` 参数时。

```python
def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):
    # ... 前向计算得到特征 x ...
    
    if targets is not None:
        # training mode: compute and return the loss
        logits = self.lm_head(x)
        # ... Logits Softcap 处理 ...
        # 计算交叉熵损失 (Cross Entropy Loss)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ...)
        return loss
```

*   **核心逻辑**：
    *   **并行计算 (Parallel)**：利用 Transformer 的并行优势，一次性计算序列中所有位置的预测结果。
    *   **Teacher Forcing**：输入完整的序列 `idx`，并利用 `targets`（通常是 `idx` 向左移一位）一次性监督所有位置的输出。
    *   **Causal Mask**：在 `CausalSelfAttention` 中，通过 `is_causal=True` 确保位置 $t$ 只能看到 $0$ 到 $t$ 的信息，无法"偷看"未来。
    *   **无 KV Cache**：训练时每次都需重新计算，不使用缓存 (`kv_cache` 为 None)。

### 2. 推理 (Inference)

推理分为两个部分：**单步前向传播**（在 `forward` 的 `else` 分支）和**自回归生成循环**（在 `generate` 方法中）。

#### A. 单步前向 (Single Step Forward)
当 `targets` 为 `None` 时，`forward` 仅返回 Logits。

```python
    else:
        # inference mode: compute and return the logits
        logits = self.lm_head(x)
        # ... Logits Softcap 处理 ...
        return logits
```

#### B. 自回归生成 (Autoregressive Generation)
**入口**：`generate` 方法。这是用户调用的顶层接口。

```python
@torch.inference_mode()
def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):
    # ... 初始化 ...
    for _ in range(max_tokens):
        # 1. 调用 forward 获取当前步的 logits
        logits = self.forward(ids) 
        
        # 2. 取最后一个时间步的输出 (next token prediction)
        logits = logits[:, -1, :] 
        
        # ... Top-K 采样 / 温度采样 ...
        
        # 3. 采样得到下一个 token id
        next_ids = torch.multinomial(probs, num_samples=1, generator=rng)
        
        # 4. 将新 token 拼接到输入中，用于下一步预测
        ids = torch.cat((ids, next_ids), dim=1)
        yield token
```

*   **核心逻辑**：
    *   **串行计算 (Sequential)**：一个接一个地生成 Token。
    *   **KV Cache (潜在优化)**：虽然 `generate` 函数目前的简单实现（Naive implementation）每次都把完整的 `ids` 传给 `forward`，导致重复计算；但底层的 `CausalSelfAttention` 已经写好了支持 `kv_cache` 的逻辑（见第 80-106 行）。

### 总结对比

| 特性 | 训练 (Training) | 推理 (Inference) |
| :--- | :--- | :--- |
| **输入** | `idx` (完整序列) + `targets` | `idx` (当前已知序列) |
| **输出** | 标量 `loss` | `logits` (概率分布) -> 采样得到 Token |
| **计算方式** | 并行 (所有 Time step 同时算) | 串行 (Time step $t \to t+1 \to \dots$) |
| **梯度** | 需要计算 (`requires_grad=True`) | 不需要 (`@torch.inference_mode()`) |
| **Attention** | 完整 Causal Mask | 依赖 KV Cache (若启用) 或重新计算 |

**一句话概括**：训练是"一次性看完全卷并打分"，推理是"写一个字，看一眼，再写下一个字"。

---

### 问题 7
用户：这份代码是dense还是moe架构？

助手：这份代码是 **Dense（稠密）** 架构，不是 MoE（Mixture of Experts，混合专家）架构。

### 证据分析

**1. MLP 层的定义**
在 `nanochat/gpt.py` 中，`MLP` 类是标准的 Transformer 前馈网络实现：

```python
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square()
        x = self.c_proj(x)
        return x
```
*   **Dense 特征**：对于每一个输入的 Token，**所有**的参数（`c_fc` 和 `c_proj` 的全部权重）都会参与计算。
*   **MoE 对比**：如果是 MoE 架构，这里会有一个 `Router` (门控网络) 和多个平行的 `Expert` (通常是多个小 MLP)，每个 Token 只会激活其中一小部分 Expert。

**2. Transformer Block 的结构**
```python
class Block(nn.Module):
    # ...
    def forward(self, x, cos_sin, kv_cache):
        x = x + self.attn(norm(x), cos_sin, kv_cache)
        x = x + self.mlp(norm(x))
        return x
```
每个层级（Layer）只有一个单一的 `mlp` 对象，没有专家路由机制。

### 结论
这就只是一个经典的、结构非常标准（除了使用了一些现代优化如 RoPE, QK Norm 等）的 **Dense Transformer** 模型。

---

### 问题 8
用户：如何通过调试器调试GPT模型，查看中间变量

助手：调试 GPT 模型最简单、最直观的方法是使用 Python 标准库中的 `pdb` (Python Debugger) 或者在 IDE (如 VS Code / Cursor) 中打断点。

### 方案一：使用 Cursor/VS Code 内置图形化调试器（推荐）

1.  **创建调试脚本**:
    创建一个名为 `debug_gpt.py` 的临时文件，写入最简单的模型运行代码：

    ```python
    # debug_gpt.py
    import torch
    from nanochat.gpt import GPT, GPTConfig

    # 1. 极简配置
    config = GPTConfig(
        vocab_size=100,
        sequence_len=16,
        n_layer=2,
        n_head=2,
        n_kv_head=2,
        n_embd=32
    )

    # 2. 初始化
    model = GPT(config)
    
    # 3. 构造输入 (Batch=1, Seq=5)
    idx = torch.randint(0, 100, (1, 5))

    # 4. 运行 (在这里打断点!)
    logits = model(idx)
    print("Output shape:", logits.shape)
    ```

2.  **设置断点**:
    *   打开 `nanochat/gpt.py`。
    *   在你感兴趣的地方（比如 `CausalSelfAttention.forward` 的 `q = self.c_q(x)...` 那一行，或者 `Block.forward`）点击行号左侧，会出现一个红点。
    
3.  **启动调试**:
    *   打开 `debug_gpt.py`。
    *   点击右上角的"运行/调试"按钮（通常是一个带虫子的三角形图标），选择 "Debug Python File"。
    *   程序会停在你打断点的地方。你可以把鼠标悬停在变量名上（如 `q`, `k`, `attn_mask`）直接查看它们的 shape 和数值。

### 方案二：使用 `pdb` 命令行调试（适用于服务器/终端环境）

在你想要暂停的代码行前插入 `import pdb; pdb.set_trace()`。

### 想要查看的关键中间变量

调试时，建议重点观察以下变量的 **Shape（形状）**，这是理解数据流的关键：

| 位置 | 变量名 | 预期形状 (Batch, Seq, Dim) | 说明 |
| :--- | :--- | :--- | :--- |
| **GPT.forward** | `idx` | `(B, T)` | 原始 Token ID |
| **Block** | `x` | `(B, T, n_embd)` | 进入 Layer 的残差流 |
| **Attention** | `q` (投影后) | `(B, n_head, T, head_dim)` | 注意这里维度的转置 |
| **Attention** | `y` (输出) | `(B, T, n_embd)` | 拼接回原来的形状 |
| **MLP** | `x` (中间) | `(B, T, 4*n_embd)` | 升维后的激活值 |
| **GPT.forward** | `logits` | `(B, T, vocab_size)` | 最终输出 |

---

### 问题 9
用户：可以基于ipynb进行调试么？

助手：可以，在 Jupyter Notebook (`.ipynb`) 中调试非常方便，而且对于查看 Tensor 的值和形状特别友好。

### 方法 1：使用 `%debug` 魔术命令（最常用，事后调试）

当代码报错（抛出 Exception）时，这是最快的方法。

1.  运行你的代码单元格，让它报错。
2.  在报错后的下一个单元格中，输入并运行：
    ```python
    %debug
    ```
3.  这会直接进入报错现场的调试模式。你可以输入 `p variable_name` 查看变量。

### 方法 2：显式设置断点 `set_trace`（主动调试）

如果你想在代码没报错时暂停查看（例如查看 `forward` 里的中间变量），可以在 Notebook 单元格里直接定义类或函数，并插入断点。

```python
from IPython.core.debugger import set_trace

# 你甚至可以 Monkey Patch (动态替换) 模型的方法来插入断点，
# 而不需要修改 gpt.py 源码文件！
original_forward = model.transformer.h[0].attn.forward

def debug_forward(x, cos_sin, kv_cache):
    print("--- 进入 Attention 层 ---")
    print(f"输入 x shape: {x.shape}")
    
    # 在这里暂停！
    set_trace() 
    
    # 继续执行原始逻辑
    return original_forward(x, cos_sin, kv_cache)

# 替换掉第一层的 forward 函数
model.transformer.h[0].attn.forward = debug_forward
```

### 方法 3：VS Code/Cursor 的 "Debug Cell" 功能

**在 Cursor/VS Code 中调试 Notebook 的最佳姿势：**

1.  确保你的 `.ipynb` 文件在 Cursor 中打开。
2.  点击单元格左侧的 **"下拉箭头"**（在运行按钮旁边）。
3.  选择 **"Debug Cell"**。
4.  程序会像普通 Python 脚本一样暂停在断点处，你可以在左侧的 "Variables" 面板直接展开查看 Tensor 的数值，非常直观。

### 总结推荐
对于 Notebook 用户，推荐顺序：
1.  **VS Code/Cursor 的 "Debug Cell" 按钮**（图形化，体验最好）。
2.  `from IPython.core.debugger import set_trace; set_trace()`（手动插入断点）。
3.  `%debug`（报错后补救）。

---

### 问题 10
用户：你最推荐的调试方式是什么？

助手：我最推荐的调试方式是 **VS Code/Cursor 原生调试器 + Jupyter Notebook ("Debug Cell")**。

### 理由

1.  **无需修改代码**：你不需要在代码里写丑陋的 `print()` 或者 `pdb.set_trace()`，保持代码库干净。
2.  **可视化变量检查**：可以直接在左侧面板看到张量（Tensor）的 Shape、Dtype 和数值。对于深度学习，盯着 Shape 变化比盯着具体数值更重要，图形化界面一眼就能看出来。
3.  **交互式 REPL**：在断点暂停时，你可以在底部的 Debug Console 随意执行 Python 代码（例如 `torch.mean(x)`），动态验证你的假设。
4.  **即用即抛**：Notebook 环境适合快速搭建 dummy input 进行测试，测试完直接关掉，不污染项目文件。

### 操作步骤

1.  打开 `dev/turing.ipynb` (或者新建一个)。
2.  写一段能够跑通 `model(input)` 的最小代码。
3.  在你怀疑有问题的代码行号左侧点击，打上 **红点（断点）**。
4.  点击 Notebook 单元格左侧运行按钮旁边的 **小箭头** -> 选择 **Debug Cell**。

这是目前调试 PyTorch 模型最高效、最优雅的方式。

